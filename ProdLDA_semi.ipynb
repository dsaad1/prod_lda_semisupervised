{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqhPcS1AyjuX"
   },
   "source": [
    "### Installs, Imports, and Pyro Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EkkTbMWyjuZ",
    "outputId": "e1cb369f-b60c-45a5-9a9f-c196a08adad5"
   },
   "outputs": [],
   "source": [
    "# 1# Run first time\n",
    "! module load cuda/9.2.88-gcc/7.1.0 cudnn/7.6.5.32-9.2-linux-x64-gcc/7.1.0-cuda9_2 anaconda3/2019.10-gcc/8.3.1\n",
    "! source activate pytorch_env\n",
    "# !pip install --up\n",
    "# !pip install pyro-ppl\n",
    "# !pip install torchvision\n",
    "# !pip install --upgrade git+https://github.com/dhudsmith/clean-the-text\n",
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W39rR1nNyjuZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import graphviz\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from ctt.clean import kitchen_sink\n",
    "\n",
    "import pyro\n",
    "from pyro import poutine\n",
    "import pyro.distributions as dist\n",
    "# import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "import torch.nn.functional as F\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import umap\n",
    "import umap.plot\n",
    "import sklearn.datasets\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LpuHjrWDyjua"
   },
   "outputs": [],
   "source": [
    "pyro.distributions.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1-TcWDrjyjua"
   },
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-Gm9gInyjua",
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fWiuASbwyjua"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "NUMBER_PAIRS = 100000\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.2\n",
    "MIN_NUM_TOKS = 2\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Dropout rate\n",
    "NUM_TOPICS = 50\n",
    "NUM_PROTOTYPES = 7\n",
    "EMBED_DIM  = 64\n",
    "HIDDEN_DIM = 128\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# Training\n",
    "adam_args = {\"lr\": 0.001, 'clip_norm':10.0, 'betas': (0.99, 0.999)}\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS = 10\n",
    "TEST_FREQUENCY = 1\n",
    "BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 2000\n",
    "OBS_SAMPLES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize W & B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwitw\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/witw/prodLDA_pairs/runs/1w2c58vz\" target=\"_blank\">warm-capybara-38</a></strong> to <a href=\"https://wandb.ai/witw/prodLDA_pairs\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"prodLDA_pairs\", entity=\"witw\", )\n",
    "wandb.run.name = \"prototype_vectors\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhrFMmkEyjub",
    "tags": []
   },
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Pd5HgcOtSejG"
   },
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='all', remove=('headers','footers','quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "shD_TpoQTC8v"
   },
   "outputs": [],
   "source": [
    "names_dict = {i:v.split('.')[0] for i,v in enumerate(newsgroups_train['target_names'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FlSMXczQTZK9"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text': newsgroups_train['data'], 'category': newsgroups_train['target']})\n",
    "df['category'] = df['category'].apply(lambda x: names_dict[x])\n",
    "df['text'] = df['text'].apply(lambda x: kitchen_sink(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lw8xddoPyjub"
   },
   "outputs": [],
   "source": [
    "# split into train/val/test\n",
    "train, test = train_test_split(df, test_size = VAL_RATIO + TEST_RATIO)\n",
    "test, val = train_test_split(test, test_size = VAL_RATIO/(VAL_RATIO + TEST_RATIO))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>+ + | kevin marshall operational support motor...</td>\n",
       "      <td>comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832</th>\n",
       "      <td>request goes medical students done planning si...</td>\n",
       "      <td>sci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4476</th>\n",
       "      <td>really stupid nitpicking capitalization rules ...</td>\n",
       "      <td>talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17368</th>\n",
       "      <td>matter assume obeying law since court order ta...</td>\n",
       "      <td>sci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7126</th>\n",
       "      <td>principles admit someone else might everyone a...</td>\n",
       "      <td>talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>checked faq first luck need convert r5 tree wi...</td>\n",
       "      <td>comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>referring mary quite problem idea mary never c...</td>\n",
       "      <td>soc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>interesting sometimes listen news seen eyes an...</td>\n",
       "      <td>talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>b b</td>\n",
       "      <td>rec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>color zenith tv sale remote control interested...</td>\n",
       "      <td>misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1885 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text category\n",
       "971    + + | kevin marshall operational support motor...     comp\n",
       "3832   request goes medical students done planning si...      sci\n",
       "4476   really stupid nitpicking capitalization rules ...     talk\n",
       "17368  matter assume obeying law since court order ta...      sci\n",
       "7126   principles admit someone else might everyone a...     talk\n",
       "...                                                  ...      ...\n",
       "756    checked faq first luck need convert r5 tree wi...     comp\n",
       "4003   referring mary quite problem idea mary never c...      soc\n",
       "2763   interesting sometimes listen news seen eyes an...     talk\n",
       "1743                                                 b b      rec\n",
       "463    color zenith tv sale remote control interested...     misc\n",
       "\n",
       "[1885 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ob6dRcZhyjuc"
   },
   "outputs": [],
   "source": [
    "# create and train vectorizer\n",
    "vectorizer = CountVectorizer(min_df=20, max_df=0.7)\n",
    "vectorizer = vectorizer.fit(train['text'].astype(str))\n",
    "\n",
    "# set vocab size now that we know it\n",
    "VOCAB_SIZE = len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0VreyOZ2aLhH"
   },
   "outputs": [],
   "source": [
    "# get bow vectors\n",
    "bows = vectorizer.transform(df.iloc[:,0].astype(str)).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7NetTWPwyjuc"
   },
   "outputs": [],
   "source": [
    "# get bow for datasets\n",
    "bows_train, bows_val, bows_test = [\n",
    "    vectorizer.transform(d.iloc[:,0].astype(str)).astype(np.int16)\n",
    "    for d in [train, val, test]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CmBnUzeYyjuc"
   },
   "outputs": [],
   "source": [
    "# find document indices where number of tokens is >=min_num_toks\n",
    "ix_keep_train, ix_keep_val, ix_keep_test = [\n",
    "    np.argwhere(np.asarray(bow.sum(axis=-1)).squeeze() >= MIN_NUM_TOKS).squeeze()\n",
    "    for bow in [bows_train, bows_val, bows_test]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MlcRiz4Hyjuc"
   },
   "outputs": [],
   "source": [
    "# subset bows to these indices\n",
    "# NOTE: these are not currently used\n",
    "bows_train = bows_train[ix_keep_train]\n",
    "bows_val = bows_train[ix_keep_val]\n",
    "bows_test = bows_train[ix_keep_test]\n",
    "\n",
    "# subset dfs for later lookups\n",
    "train = train.iloc[ix_keep_train]\n",
    "val = train.iloc[ix_keep_val]\n",
    "test = train.iloc[ix_keep_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeQ3XOC5yjud",
    "outputId": "245d0dac-54e5-4ed2-99d1-2a5408e52bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(num_samples, vocab_size) (num_samples, num_columns) min_num_toks\n",
      "Train: (12680, 6373) (12680, 2) 2\n",
      "Val: (1806, 6373) (1806, 2) 2\n",
      "Test: (3628, 6373) (3628, 2) 2\n"
     ]
    }
   ],
   "source": [
    "# make sure our subsetting worked correctly\n",
    "print(\"(num_samples, vocab_size) (num_samples, num_columns) min_num_toks\")\n",
    "print(\"Train:\", bows_train.shape, train.shape, bows_train.sum(axis=-1).min())\n",
    "print(\"Val:\", bows_val.shape, val.shape, bows_val.sum(axis=-1).min())\n",
    "print(\"Test:\", bows_test.shape, test.shape, bows_test.sum(axis=-1).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "E4HRtpAEVSkS"
   },
   "outputs": [],
   "source": [
    "def get_pairs_table(df):\n",
    "    pair_ix = np.random.choice(df.index.values, size=(NUMBER_PAIRS,2), replace=True)\n",
    "    catsA = df.category[pair_ix[:,0]]\n",
    "    catsB = df.category[pair_ix[:,1]]\n",
    "\n",
    "    top_cat_A = catsA.str.split('.').apply(lambda x: x[0])\n",
    "    top_cat_B = catsB.str.split('.').apply(lambda x: x[0])\n",
    "\n",
    "    is_similar = (top_cat_A.values == top_cat_B.values)\n",
    "\n",
    "    return pd.DataFrame({'ix_A': pair_ix[:,0],\n",
    "                         'ix_B': pair_ix[:,1],\n",
    "                         'label': is_similar})\n",
    "\n",
    "train_pairs, val_pairs, test_pairs = [\n",
    "                                      get_pairs_table(d)\n",
    "                                      for d in [train, val, test]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhSnYHuxyjud",
    "tags": []
   },
   "source": [
    "# Custom Dataloader/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "cvq3v0FKyjud"
   },
   "outputs": [],
   "source": [
    "class DocumentPairData(Dataset):\n",
    "\n",
    "    def __init__(self, bows, index_table, prob=0.5):\n",
    "        \"\"\"\n",
    "        Integrated dataset loader (supervised and unsupervised dataset)\n",
    "        \n",
    "        Args:\n",
    "            data: an pre-loaded sparse matrix\n",
    "            index_table: dataframe/list of pre-split pairwise indices, where index_table[i] = [a_ix, b_ix, label]\n",
    "            prob (float): between [0,1], the probability of returning a supervised sample\n",
    "\n",
    "        Return:\n",
    "            {'a': bow, 'b': bow, 'label': bool, observed: bool}\n",
    "        \"\"\"\n",
    "        self.bows = bows\n",
    "        self.index_table = index_table\n",
    "        self.prob = prob\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.index_table)\n",
    "\n",
    "    def __getitem__(self, ix): \n",
    "        \n",
    "        a_ix = self.index_table.iloc[ix][0]\n",
    "        b_ix = self.index_table.iloc[ix][1]\n",
    "        label = self.index_table.iloc[ix][2]\n",
    "                \n",
    "        a = self.bows[a_ix].toarray().astype(np.float32)\n",
    "        b = self.bows[b_ix].toarray().astype(np.float32)\n",
    "        \n",
    "        is_observed = (np.random.rand() < self.prob)\n",
    "        \n",
    "        return {'a': a, 'b': b, 'label': label, 'observed': is_observed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oHW-3swTyjue"
   },
   "outputs": [],
   "source": [
    "# datasets\n",
    "data_train, data_val, data_test = [\n",
    "    DocumentPairData(bows=bows, index_table=ix_table, prob=0.5)\n",
    "    for ix_table in [train_pairs, val_pairs, test_pairs]\n",
    "]\n",
    "\n",
    "# dataloaders\n",
    "dl_train = DataLoader(data_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "dl_val = DataLoader(data_val, batch_size=VAL_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "dl_test = DataLoader(data_test, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6zagxADeuKY",
    "outputId": "e7f68f05-53c8-4ed9-9b2f-9a79125c736f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : torch.Size([128, 1, 6373]) torch.float32\n",
      "b : torch.Size([128, 1, 6373]) torch.float32\n",
      "label : torch.Size([128]) torch.bool\n",
      "observed : torch.Size([128]) torch.bool\n"
     ]
    }
   ],
   "source": [
    "for k, v in next(iter(dl_train)).items():\n",
    "    print(k,\":\", v.shape, v.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XqJO3eSyjue",
    "tags": []
   },
   "source": [
    "# ETM Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uWwiXiVsyjue"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # Base class for the encoder net, used in the guide\n",
    "    def __init__(self, vocab_size, num_topics, hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout)  # to avoid component collapse\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fcmu = nn.Linear(hidden, num_topics)\n",
    "        self.fclv = nn.Linear(hidden, num_topics)\n",
    "        # NB: here we set `affine=False` to reduce the number of learning parameters\n",
    "        # See https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        # for the effect of this flag in BatchNorm1d\n",
    "        self.bnmu = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "        self.bnlv = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = F.softplus(self.fc1(inputs))\n",
    "        h = F.softplus(self.fc2(h))\n",
    "        h = self.drop(h)\n",
    "        # Î¼ and Î£ are the outputs\n",
    "        logtheta_loc = self.bnmu(self.fcmu(h))\n",
    "        logtheta_logvar = self.bnlv(self.fclv(h))\n",
    "        logtheta_scale = (0.5 * logtheta_logvar).exp()  # Enforces positivity\n",
    "        return logtheta_loc, logtheta_scale+0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-AiIRhlWyjue"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    # Base class for the decoder net, used in the model\n",
    "    def __init__(self, vocab_size, num_topics, dropout):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Linear(num_topics, vocab_size, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.drop(inputs)\n",
    "        # the output is Ïƒ(Î²Î¸)\n",
    "        return F.softmax(self.bn(self.beta(inputs)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "NUs1aoCayjue"
   },
   "outputs": [],
   "source": [
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, vocab_size, num_topics, hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_topics = num_topics\n",
    "        self.encoder = Encoder(vocab_size, num_topics, hidden, dropout).to(DEVICE)\n",
    "        self.decoder = Decoder(vocab_size, num_topics, dropout).to(DEVICE)\n",
    "\n",
    "    def model(self, docs_a, docs_b, c, x=None):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"documents_a\", docs_a.shape[0]):\n",
    "            # Dirichlet prior ð‘(ðœƒ|ð›¼) is replaced by a logistic-normal distribution\n",
    "            logtheta_loc_a = docs_a.new_zeros((docs_a.shape[0], self.num_topics))\n",
    "            logtheta_scale_a = docs_a.new_ones((docs_a.shape[0], self.num_topics))\n",
    "            logtheta_a = pyro.sample(\n",
    "                \"logtheta_a\", dist.Normal(logtheta_loc_a, logtheta_scale_a).to_event(1))\n",
    "            theta_a = F.softmax(logtheta_a, -1)\n",
    "\n",
    "            # conditional distribution of ð‘¤ð‘› is defined as\n",
    "            # ð‘¤ð‘›|ð›½,ðœƒ ~ Categorical(ðœŽ(ð›½ðœƒ))\n",
    "            count_param_a = self.decoder(theta_a)\n",
    "            # Currently, PyTorch Multinomial requires `total_count` to be homogeneous.\n",
    "            # Because the numbers of words across documents can vary,\n",
    "            # we will use the maximum count accross documents here.\n",
    "            # This does not affect the result because Multinomial.log_prob does\n",
    "            # not require `total_count` to evaluate the log probability.\n",
    "            total_count_a = int(docs_a.sum(-1).max())\n",
    "            pyro.sample(\n",
    "                'obs_a',\n",
    "                dist.Multinomial(total_count_a, count_param_a),\n",
    "                obs=docs_a\n",
    "            )\n",
    "        with pyro.plate(\"documents_b\", docs_b.shape[0]):\n",
    "            # Dirichlet prior ð‘(ðœƒ|ð›¼) is replaced by a logistic-normal distribution\n",
    "            logtheta_loc_b = docs_b.new_zeros((docs_b.shape[0], self.num_topics))\n",
    "            logtheta_scale_b = docs_b.new_ones((docs_b.shape[0], self.num_topics))\n",
    "            logtheta_b = pyro.sample(\n",
    "                \"logtheta_b\", dist.Normal(logtheta_loc_b, logtheta_scale_b).to_event(1))\n",
    "            theta_b = F.softmax(logtheta_b, -1)\n",
    "\n",
    "            # conditional distribution of ð‘¤ð‘› is defined as\n",
    "            # ð‘¤ð‘›|ð›½,ðœƒ ~ Categorical(ðœŽ(ð›½ðœƒ))\n",
    "            count_param_b = self.decoder(theta_b)\n",
    "            # Currently, PyTorch Multinomial requires `total_count` to be homogeneous.\n",
    "            # Because the numbers of words across documents can vary,\n",
    "            # we will use the maximum count accross documents here.\n",
    "            # This does not affect the result because Multinomial.log_prob does\n",
    "            # not require `total_count` to evaluate the log probability.\n",
    "            total_count_b = int(docs_b.sum(-1).max())\n",
    "            pyro.sample(\n",
    "                'obs_b',\n",
    "                dist.Multinomial(total_count_b, count_param_b),\n",
    "                obs=docs_b\n",
    "            )\n",
    "        \n",
    "        if x is not None:\n",
    "            with pyro.plate('pairs', x.shape[0]):\n",
    "                pyro.sample(\n",
    "                        'obs_c',\n",
    "                        dist.Bernoulli(logits=x),\n",
    "                        obs=c\n",
    "                    )\n",
    "\n",
    "    def guide(self, docs_a, docs_b, c, x=None):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"documents_a\", docs_a.shape[0]):\n",
    "            # Dirichlet prior ð‘(ðœƒ|ð›¼) is replaced by a logistic-normal distribution,\n",
    "            # where Î¼ and Î£ are the encoder network outputs\n",
    "            logtheta_loc_a, logtheta_scale_a = self.encoder(docs_a)\n",
    "            logtheta_a = pyro.sample(\n",
    "                \"logtheta_a\", dist.Normal(logtheta_loc_a, logtheta_scale_a).to_event(1))\n",
    "        with pyro.plate(\"documents_b\", docs_b.shape[0]): \n",
    "            # Dirichlet prior ð‘(ðœƒ|ð›¼) is replaced by a logistic-normal distribution,\n",
    "            # where Î¼ and Î£ are the encoder network outputs\n",
    "            logtheta_loc_b, logtheta_scale_b = self.encoder(docs_b)\n",
    "            logtheta_b = pyro.sample(\n",
    "                \"logtheta_b\", dist.Normal(logtheta_loc_b, logtheta_scale_b).to_event(1)) \n",
    "        \n",
    "        p = pyro.param('p', torch.randn(NUM_PROTOTYPES, NUM_TOPICS, device=DEVICE))\n",
    "        norm = (p**2).sum(axis=-1).sqrt().unsqueeze(-1)\n",
    "        p = p / norm\n",
    "        \n",
    "        a = pyro.param('a', torch.tensor(1.0, device=DEVICE))\n",
    "        b = pyro.param('b', torch.tensor(0.0, device=DEVICE))\n",
    "        \n",
    "        \n",
    "        sA = logtheta_loc_a @ p.t()\n",
    "        sB = logtheta_loc_b @ p.t()\n",
    "        x =  pyro.deterministic(\"x\", a * (sA * sB).sum(dim=-1) + b)\n",
    "        return x\n",
    "\n",
    "    def beta(self):\n",
    "        # beta matrix elements are the weights of the FC layer on the decoder\n",
    "        return self.decoder.beta.weight.detach().T\n",
    "    \n",
    "    def calc_kl_divergence(self, x):\n",
    "        # Calculate KL Divergence of latent document distributions\n",
    "        z_loc, z_sigma = self.encoder(x)\n",
    "        size = z_loc.shape[1]\n",
    "        kl_loss = torch.sum(\n",
    "            (-torch.log(z_sigma) + (torch.square(z_sigma) + torch.square(z_loc))/2 - 1/2), (0, 1))\n",
    "\n",
    "        return kl_loss\n",
    "    \n",
    "    def reconstruct_doc(self, x):\n",
    "        x=x.to(DEVICE).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z_loc, _ = self.encoder(x)\n",
    "            theta = F.softmax(z_loc, dim=-1)\n",
    "            word_probs = self.decoder(theta)\n",
    "                              \n",
    "        return word_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this custom elbo allows us to pass deterministic values from the guide \n",
    "# into the model\n",
    "def custom_elbo(model, guide, *args, **kwargs):\n",
    "    # run the guide and trace its execution\n",
    "    guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n",
    "\n",
    "    # get the 'x_loc' value from the guide trace\n",
    "    x = guide_trace.nodes['x']['value']\n",
    "\n",
    "    # run the model and replay it against the samples from the guide\n",
    "    # notice where x_loc is passed into the model\n",
    "    model_trace = poutine.trace(\n",
    "        poutine.replay(model, trace=guide_trace)).get_trace(x = x, *args, **kwargs)\n",
    "    # construct the elbo loss function\n",
    "    return -1*(model_trace.log_prob_sum() - guide_trace.log_prob_sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fk2anUsCyjuf"
   },
   "source": [
    "# Training and Evaluation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "q-wenFNRyjuf"
   },
   "outputs": [],
   "source": [
    "def train(svi, train_loader, device, progress_interval = 100, print_debug = False):\n",
    "    topic_model.train()\n",
    "    \n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    train_acc = 0.0\n",
    "    train_auroc = 0.0\n",
    "    batches = 0\n",
    "    \n",
    "    \n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for i, x in enumerate(train_loader):\n",
    "        batches += 1\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        x_a = x['a'].to(device).squeeze()\n",
    "        x_b = x['b'].to(device).squeeze()\n",
    "        x_label = x['label'].to(device).type(torch.float32)\n",
    "        \n",
    "        if print_debug and i % progress_interval == 0:\n",
    "            print(f\"Step {i}; avg. loss {epoch_loss/(i+1)}\", end='\\r')\n",
    "        \n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x_a, x_b, x_label)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = svi.guide(x_a, x_b, x_label)\n",
    "\n",
    "        x_label = x_label.cpu()\n",
    "        logits = x.cpu()\n",
    "#         # do something with the log odds\n",
    "        logits_binned = np.digitize(logits, [0.5], right=False)\n",
    "        train_acc += accuracy_score(x_label, logits_binned)\n",
    "        train_auroc += roc_auc_score(x_label, logits)\n",
    "\n",
    "        \n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    epoch_acc = train_acc / batches\n",
    "    epoch_auroc = train_auroc / batches\n",
    "    return total_epoch_loss_train, epoch_acc, epoch_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "56ugiYHHyjuf"
   },
   "outputs": [],
   "source": [
    "def evaluate(svi, etm, test_loader, device): \n",
    "    etm.eval()\n",
    "    \n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.0\n",
    "    test_ce_loss = 0.0\n",
    "    test_kl_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_auroc = 0.0\n",
    "    batches = 0\n",
    "    \n",
    "    # compute the loss over the entire test set\n",
    "    for x in test_loader:\n",
    "        batches += 1\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        x_a = x['a'].to(device).squeeze()\n",
    "        x_b = x['b'].to(device).squeeze()\n",
    "        x_label = x['label'].to(device).type(torch.float32)\n",
    "            \n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x_a, x_b, x_label)\n",
    "        \n",
    "        # generate reconstruction of batch documents and move to cuda if designated\n",
    "        with torch.no_grad():\n",
    "            recon_x_a = etm.reconstruct_doc(x_a)\n",
    "            recon_x_b = etm.reconstruct_doc(x_b)\n",
    "            logits = svi.guide(x_a, x_b, x_label)\n",
    "\n",
    "        x_label = x_label.cpu()\n",
    "        logits = logits.cpu()\n",
    "#         # do something with the log odds\n",
    "        logits_binned = np.digitize(logits, [0.3], right=False)\n",
    "        test_acc += accuracy_score(x_label, logits_binned)\n",
    "        test_auroc += roc_auc_score(x_label, logits)\n",
    "\n",
    "        # calculate and sum cross entropy loss and kl divergence\n",
    "        x_a=x_a.squeeze()\n",
    "        x_b=x_b.squeeze()\n",
    "        log_probs_a = torch.log(recon_x_a)\n",
    "        log_probs_b = torch.log(recon_x_b)\n",
    "        targets_a = x_a/(x_a.sum(axis=-1)[:,None])\n",
    "        targets_b = x_b/(x_b.sum(axis=-1)[:,None])\n",
    "        ce_loss_a = F.cross_entropy(log_probs_a, targets_a, reduction='sum')\n",
    "        ce_loss_b = F.cross_entropy(log_probs_b, targets_b, reduction='sum')\n",
    "        test_ce_loss += ce_loss_a + ce_loss_b\n",
    "        test_kl_loss += etm.calc_kl_divergence(x_a.squeeze(1)) + etm.calc_kl_divergence(x_b.squeeze(1))\n",
    "\n",
    "        \n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    epoch_elbo = test_loss / normalizer_test\n",
    "    epoch_ce_loss = test_ce_loss / normalizer_test\n",
    "    epoch_kl_loss = test_kl_loss / normalizer_test\n",
    "    epoch_acc = test_acc / batches\n",
    "    epoch_auroc = test_auroc / batches\n",
    "    \n",
    "    return epoch_elbo, epoch_ce_loss.item(), epoch_kl_loss.item(), epoch_acc, epoch_auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsaotgwIyjuf"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "i9SVXdGFyjuf"
   },
   "outputs": [],
   "source": [
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the VAE\n",
    "topic_model = ProdLDA(VOCAB_SIZE, NUM_TOPICS, HIDDEN_DIM, DROPOUT_RATE).to(DEVICE)\n",
    "\n",
    "# setup the optimizer\n",
    "optimizer = ClippedAdam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(topic_model.model, topic_model.guide, optimizer, loss=custom_elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBxPY0pHyjuf",
    "outputId": "03a9e506-6feb-4b0f-cde7-072d6afa0f4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Trace Shapes:             \n",
      "         Param Sites:             \n",
      "decoder$$$beta.weight 6373 50     \n",
      "        Sample Sites:             \n",
      "     documents_a dist       |     \n",
      "                value  128  |     \n",
      "      logtheta_a dist  128  |   50\n",
      "                value  128  |   50\n",
      "           obs_a dist  128  | 6373\n",
      "                value  128  | 6373\n",
      "     documents_b dist       |     \n",
      "                value  128  |     \n",
      "      logtheta_b dist  128  |   50\n",
      "                value  128  |   50\n",
      "           obs_b dist  128  | 6373\n",
      "                value  128  | 6373\n"
     ]
    }
   ],
   "source": [
    "batch_1 = next(iter(dl_train))\n",
    "dat_a = batch_1['a'].to(DEVICE).squeeze()\n",
    "dat_b = batch_1['b'].to(DEVICE).squeeze()\n",
    "dat_label = batch_1['label'].to(DEVICE).squeeze().type(torch.float32)\n",
    "\n",
    "print(pyro.poutine.trace(topic_model.model).get_trace(dat_a, dat_b, dat_label).format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5WGxoYPyjuf",
    "outputId": "7172cec2-328a-4010-fdde-18c84c92b04c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Trace Shapes:             \n",
      "         Param Sites:             \n",
      " encoder$$$fc1.weight 128 6373    \n",
      "   encoder$$$fc1.bias      128    \n",
      " encoder$$$fc2.weight 128  128    \n",
      "   encoder$$$fc2.bias      128    \n",
      "encoder$$$fcmu.weight  50  128    \n",
      "  encoder$$$fcmu.bias       50    \n",
      "encoder$$$fclv.weight  50  128    \n",
      "  encoder$$$fclv.bias       50    \n",
      "                    p   7   50    \n",
      "                    a             \n",
      "                    b             \n",
      "        Sample Sites:             \n",
      "     documents_a dist        |    \n",
      "                value 128    |    \n",
      "      logtheta_a dist 128    |  50\n",
      "                value 128    |  50\n",
      "     documents_b dist        |    \n",
      "                value 128    |    \n",
      "      logtheta_b dist 128    |  50\n",
      "                value 128    |  50\n",
      "               x dist        | 128\n",
      "                value        | 128\n"
     ]
    }
   ],
   "source": [
    "print(pyro.poutine.trace(topic_model.guide).get_trace(dat_a, dat_b, dat_label).format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "id": "LiMk1N3_yjuf",
    "outputId": "3b947702-bd00-4ec9-d45c-76ec119d5094"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"390pt\" height=\"187pt\"\n",
       " viewBox=\"0.00 0.00 389.50 187.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 183)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-183 385.5,-183 385.5,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_documents_a</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"8,-8 8,-171 118,-171 118,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"73.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">documents_a</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_documents_b</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"126,-8 126,-171 238,-171 238,-8 126,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"193.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">documents_b</text>\n",
       "</g>\n",
       "<!-- logtheta_a -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>logtheta_a</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"63\" cy=\"-145\" rx=\"47.3916\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-141.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">logtheta_a</text>\n",
       "</g>\n",
       "<!-- obs_a -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>obs_a</title>\n",
       "<ellipse fill=\"#c0c0c0\" stroke=\"#000000\" cx=\"63\" cy=\"-57\" rx=\"31.3957\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">obs_a</text>\n",
       "</g>\n",
       "<!-- logtheta_a&#45;&gt;obs_a -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>logtheta_a&#45;&gt;obs_a</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M63,-126.7663C63,-114.8492 63,-99.0384 63,-85.4817\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"66.5001,-85.2103 63,-75.2103 59.5001,-85.2103 66.5001,-85.2103\"/>\n",
       "</g>\n",
       "<!-- logtheta_b -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>logtheta_b</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"182\" cy=\"-145\" rx=\"48.1917\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"182\" y=\"-141.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">logtheta_b</text>\n",
       "</g>\n",
       "<!-- obs_b -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>obs_b</title>\n",
       "<ellipse fill=\"#c0c0c0\" stroke=\"#000000\" cx=\"182\" cy=\"-57\" rx=\"31.6951\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"182\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">obs_b</text>\n",
       "</g>\n",
       "<!-- logtheta_b&#45;&gt;obs_b -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>logtheta_b&#45;&gt;obs_b</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M182,-126.7663C182,-114.8492 182,-99.0384 182,-85.4817\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"185.5001,-85.2103 182,-75.2103 178.5001,-85.2103 185.5001,-85.2103\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"256.5\" y=\"-163.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">logtheta_a ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"256.5\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">obs_a ~ Multinomial</text>\n",
       "<text text-anchor=\"start\" x=\"256.5\" y=\"-133.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">logtheta_b ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"256.5\" y=\"-118.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">obs_b ~ Multinomial</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x149348ceb730>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.render_model(topic_model.model, model_args=(dat_a, dat_b, dat_label, ), render_distributions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "dca9aYzyyjug",
    "outputId": "eda19553-8496-4201-a3fa-ceca51663fc1"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"444pt\" height=\"99pt\"\n",
       " viewBox=\"0.00 0.00 443.50 99.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 95)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-95 439.5,-95 439.5,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_documents_a</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"64,-8 64,-83 174,-83 174,-8 64,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"129.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">documents_a</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_documents_b</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"182,-8 182,-83 294,-83 294,-8 182,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"249.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">documents_b</text>\n",
       "</g>\n",
       "<!-- x -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x</title>\n",
       "<ellipse fill=\"#c0c0c0\" stroke=\"#000000\" cx=\"27\" cy=\"-57\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x</text>\n",
       "</g>\n",
       "<!-- logtheta_a -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>logtheta_a</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"119\" cy=\"-57\" rx=\"47.3916\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"119\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">logtheta_a</text>\n",
       "</g>\n",
       "<!-- logtheta_b -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>logtheta_b</title>\n",
       "<ellipse fill=\"#ffffff\" stroke=\"#000000\" cx=\"238\" cy=\"-57\" rx=\"48.1917\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"238\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">logtheta_b</text>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"312.5\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">logtheta_a ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"312.5\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">logtheta_b ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"312.5\" y=\"-38.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x ~ Delta</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x149348b010a0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.render_model(topic_model.guide, model_args=(dat_a, dat_b, dat_label, ), render_distributions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = topic_model.guide(dat_a, dat_b, dat_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num samples 10, do auroc/accuracy for all, average results, show sd\n",
    "# use scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vHKVjjOCyjug",
    "outputId": "ea1176e6-2e15-4934-f8ff-d1bfb18a0b31",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training\n",
      "[epoch 000]  average training loss: 783.6187\n",
      "\n",
      "Evaluation: \n",
      "[epoch 000]  average elbo loss: 751.2572\n",
      "              average ce loss:   17.4610\n",
      "              average kld loss:  75.9617\n",
      "              average accuracy:     0.3849\n",
      "              average auroc:  0.4946\n",
      "\n",
      "[epoch 001]  average training loss: 700.3716\n",
      "\n",
      "Evaluation: \n",
      "[epoch 001]  average elbo loss: 694.9944\n",
      "              average ce loss:   17.1275\n",
      "              average kld loss:  59.5783\n",
      "              average accuracy:     0.8057\n",
      "              average auroc:  0.5006\n",
      "\n",
      "[epoch 002]  average training loss: 681.0651\n",
      "\n",
      "Evaluation: \n",
      "[epoch 002]  average elbo loss: 680.1229\n",
      "              average ce loss:   16.8921\n",
      "              average kld loss:  63.5616\n",
      "              average accuracy:     0.8057\n",
      "              average auroc:  0.5064\n",
      "\n",
      "[epoch 003]  average training loss: 673.8688\n",
      "\n",
      "Evaluation: \n",
      "[epoch 003]  average elbo loss: 679.4166\n",
      "              average ce loss:   16.8775\n",
      "              average kld loss:  68.4280\n",
      "              average accuracy:     0.8057\n",
      "              average auroc:  0.5119\n",
      "\n",
      "Step 600; avg. loss 86132.24938903919\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/local_scratch/pbs.4793357.pbs02/ipykernel_69591/777768317.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     total_epoch_loss_train, train_acc, train_auroc = train(svi, dl_train, device=DEVICE, print_debug=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtotal_epoch_loss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_debug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_elbo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtotal_epoch_loss_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_scratch/pbs.4793357.pbs02/ipykernel_69591/328367720.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(svi, train_loader, device, progress_interval, print_debug)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# do a training epoch over each mini-batch x returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# by the data loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mbatches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# if on GPU put mini-batch into CUDA memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_scratch/pbs.4793357.pbs02/ipykernel_69591/2927586171.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ix)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0ma_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mb_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1566\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3377\u001b[0m         \u001b[0;31m# irow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3379\u001b[0;31m             \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3381\u001b[0m             \u001b[0;31m# if we are a copy, mark as such\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;31m# result[blk.mgr_locs] = blk._slice((slice(None), loc))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionDtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36miget\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_elbo = []\n",
    "test_elbo = []\n",
    "test_celoss = []\n",
    "test_klloss = []\n",
    "print(\"Beginning Training\")\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "#     total_epoch_loss_train, train_acc, train_auroc = train(svi, dl_train, device=DEVICE, print_debug=True)\n",
    "    total_epoch_loss_train, _, _ = train(svi, dl_train, device=DEVICE, print_debug=True)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "            \n",
    "#     wandb.log({'epoch': epoch,\n",
    "#                'train_acc': train_acc,\n",
    "#                'train_auroc': train_auroc,\n",
    "#                'train_elbo': total_epoch_loss_train})\n",
    "    \n",
    "    wandb.log({'epoch': epoch,\n",
    "               'train_elbo': total_epoch_loss_train})\n",
    "    \n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "#     print(\"              average accuracy:         %.4f\" % (train_acc))\n",
    "#     print(\"              average auroc:      %.4f\" % (train_auroc))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test, total_epoch_celoss_test, total_epoch_klloss_test, test_acc, test_auroc = evaluate(svi, topic_model, dl_val, device=DEVICE)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        test_celoss.append(total_epoch_celoss_test)\n",
    "        test_klloss.append(total_epoch_klloss_test)\n",
    "#         print(x)\n",
    "        wandb.log({'epoch': epoch,\n",
    "                   'test_acc': test_acc,\n",
    "                   'test_auroc': test_auroc,\n",
    "                   'test_elbo': total_epoch_loss_test,\n",
    "                   'test_entropy': total_epoch_celoss_test,\n",
    "                   'test_kl': total_epoch_klloss_test})\n",
    "        \n",
    "        print(\"\\nEvaluation: \")\n",
    "        print(\"[epoch %03d]  average elbo loss: %.4f\" % (epoch, total_epoch_loss_test))\n",
    "        print(\"              average ce loss:   %.4f\" % (total_epoch_celoss_test))\n",
    "        print(\"              average kld loss:  %.4f\" % (total_epoch_klloss_test))\n",
    "        print(\"              average accuracy:     %.4f\" % (test_acc))\n",
    "        print(\"              average auroc:  %.4f\\n\" % (test_auroc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCdIqCNOyjug"
   },
   "source": [
    "# Convergence Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86z6HslMyjug"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,len(test_elbo),TEST_FREQUENCY), test_elbo)\n",
    "plt.title('ELBO over Epochs')\n",
    "plt.ylabel('Test ELBO')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwuJBzI8yjug"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,len(test_elbo),TEST_FREQUENCY), test_celoss)\n",
    "plt.title('Cross Entropy Loss over Epochs')\n",
    "plt.ylabel('Test Cross Entropy Loss')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWjHh6OByjug"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,len(test_elbo),TEST_FREQUENCY), test_klloss)\n",
    "plt.title('KL Divergence over Epochs')\n",
    "plt.ylabel('Test KL Divergence Loss')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTppgysUyjug",
    "tags": []
   },
   "source": [
    "# Test Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clfqQwWzyjug",
    "tags": []
   },
   "source": [
    "### Reconstructions/Topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqUs489Wyjug"
   },
   "outputs": [],
   "source": [
    "NUM_MINI_TEST = 5\n",
    "TOP_N = 20\n",
    "tok2ix = vectorizer.vocabulary_\n",
    "ix2tok = {v:k for k,v in tok2ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iW0XsSkMyjug"
   },
   "outputs": [],
   "source": [
    "def test_recon(sentences = List[str], num_keep=10):\n",
    "    bows = torch.tensor(vectorizer.transform(sentences).toarray(), dtype=torch.float32, device='cuda:0')\n",
    "\n",
    "    recon = topic_model.reconstruct_doc(bows).detach()\n",
    "    ixs = torch.argsort(recon, axis=-1, descending=True)  \n",
    "    ixs = ixs[:,:num_keep]\n",
    "    \n",
    "    terms_list = []\n",
    "    for jx, doc in enumerate(ixs):\n",
    "        terms = [(ix2tok[ix.item()], recon[jx, ix].item()) for ix in doc]\n",
    "        terms_list.append(terms)\n",
    "    \n",
    "    return terms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_CPTpSWyjug"
   },
   "outputs": [],
   "source": [
    "# lengthen\n",
    "test_recon(['computer graphic cards for sale',\n",
    "            'left is liberal, right is conservative'\n",
    "           ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBEO3PoEyjuh",
    "tags": []
   },
   "source": [
    "### Topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQDj6AtLyjuh"
   },
   "outputs": [],
   "source": [
    "NUM_MINI_TOPICS = 5\n",
    "START_TOPIC_I = 5\n",
    "TOP_N_TOPIC = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MNKBHK1yjuh"
   },
   "outputs": [],
   "source": [
    "betas = topic_model.beta()\n",
    "top_term_ixs = betas.argsort(axis=-1, descending=True)[:,:TOP_N_TOPIC]\n",
    "\n",
    "topic_terms = []\n",
    "for jx, topic in enumerate(top_term_ixs):\n",
    "    terms = [(ix2tok[ix.item()], betas[jx, ix].item()) for ix in topic]\n",
    "    topic_terms.append(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mmt1djsByjuh"
   },
   "outputs": [],
   "source": [
    "topic_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4fGsWyqyjuh"
   },
   "outputs": [],
   "source": [
    "# torch.save({\"model\" : topic_model.state_dict(), \"guide\" : svi.guide}, \"../data/mymodel.pt\")\n",
    "# pyro.get_param_store().save(\"../data/mymodelparams.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dict = torch.load(\"../data/pretrained_lda.pt\")\n",
    "topic_model.load_state_dict(saved_model_dict['model'])\n",
    "svi.guide = saved_model_dict['guide']\n",
    "pyro.get_param_store().load(\"../data/pretrained_params.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dict['model']['encoder.fc1.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.state_dict()['encoder.fc1.weight'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get topic reps for validation set\n",
    "x = torch.tensor(bows_val.toarray()).to(DEVICE).type(torch.float)\n",
    "with torch.no_grad():\n",
    "    z_loc, z_scale = topic_model.encoder(x)\n",
    "    \n",
    "features = z_loc.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.softmax(features, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = umap.UMAP(n_neighbors=15, random_state=42, min_dist=0.1).fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.transform(features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pyro.get_param_store()['p'].detach().cpu()\n",
    "phi = torch.softmax(p, axis=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_2d = trans.transform(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(trans, labels=val.category,theme='fire')\n",
    "plt.scatter(x=p_2d[:,0], y=p_2d[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ProdLDA_semi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
