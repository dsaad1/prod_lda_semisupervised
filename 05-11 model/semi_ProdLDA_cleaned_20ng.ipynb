{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqhPcS1AyjuX"
   },
   "source": [
    "### Installs, Imports, and Pyro Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EkkTbMWyjuZ",
    "outputId": "e1cb369f-b60c-45a5-9a9f-c196a08adad5"
   },
   "outputs": [],
   "source": [
    "# 1# Run first time\n",
    "# ! module load anaconda3/2022.05-gcc/9.5.0 cuda/11.1.1-gcc/9.5.0 cudnn/8.0.5.39-11.1-gcc/9.5.0-cu11_1\n",
    "# ! source activate updated_pytorch\n",
    "# !pip install --up\n",
    "# !pip install pyro-ppl\n",
    "# !pip install torchvision\n",
    "# !pip install --upgrade git+https://github.com/dhudsmith/clean-the-text\n",
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W39rR1nNyjuZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsaad/.conda/envs/updated_pytorch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset import PairwiseData, DocumentPairData\n",
    "from model import Encoder, Decoder, ProdLDA, custom_elbo\n",
    "import os\n",
    "import pickle as pkl\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import pyro\n",
    "from pyro import poutine\n",
    "import pyro.distributions as dist\n",
    "# import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "import torch.nn.functional as F\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import umap\n",
    "import umap.plot\n",
    "import sklearn.datasets\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LpuHjrWDyjua"
   },
   "outputs": [],
   "source": [
    "pyro.distributions.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1-TcWDrjyjua"
   },
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-Gm9gInyjua",
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fWiuASbwyjua"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "NUMBER_PAIRS = 100000\n",
    "\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Dropout rate\n",
    "NUM_TOPICS = 50\n",
    "NUM_PROTOTYPES = 5\n",
    "EMBED_DIM  = 64\n",
    "HIDDEN_DIM = 128\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# Training\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS = 75\n",
    "TEST_FREQUENCY = 1\n",
    "BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 2000\n",
    "OBS_SAMPLES = 10\n",
    "\n",
    "# gamma = 0.25\n",
    "# num_steps = (NUMBER_PAIRS // BATCH_SIZE) * NUM_EPOCHS\n",
    "# lrd = np.exp(np.log(gamma)/num_steps)\n",
    "adam_args = {\"lr\": 0.003, 'clip_norm':10.0, 'betas': (0.99, 0.999)}\n",
    "\n",
    "latest_plotted_p = torch.zeros(NUM_PROTOTYPES, NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhrFMmkEyjub",
    "tags": []
   },
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oHW-3swTyjue"
   },
   "outputs": [],
   "source": [
    "def get_data(pair_percentage=1.0):\n",
    "    # pairwise data\n",
    "    pwd = PairwiseData()\n",
    "    train_pairs, val_pairs, test_pairs = [pwd.get_pairs_table(d) for d in [pwd.train, pwd.val, pwd.test]]\n",
    "\n",
    "    # datasets\n",
    "    data_train, data_val, data_test = [\n",
    "        DocumentPairData(bows=pwd.bows, index_table=ix_table, prob=pair_percentage)\n",
    "        for ix_table in [train_pairs, val_pairs, test_pairs]\n",
    "    ]\n",
    "\n",
    "    # dataloaders\n",
    "    dl_train = DataLoader(data_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "    dl_val = DataLoader(data_val, batch_size=VAL_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "    dl_test = DataLoader(data_test, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "    \n",
    "    return pwd, dl_train, dl_val, dl_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fk2anUsCyjuf"
   },
   "source": [
    "# Training and Evaluation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "q-wenFNRyjuf"
   },
   "outputs": [],
   "source": [
    "def train(svi, topic_model, train_loader, device, progress_interval = 100, print_debug = False, epoch = 0, latest_plotted_p = None, trans = None):\n",
    "    topic_model.train()\n",
    "    \n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    logits_epoch = []\n",
    "    labels_epoch = []\n",
    "    batches = 0\n",
    "    \n",
    "    \n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for i, x in enumerate(train_loader):\n",
    "        batches += 1\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        x_a = x['a'].to(device).squeeze()\n",
    "        x_b = x['b'].to(device).squeeze()\n",
    "        x_label = x['label'].to(device).type(torch.float32)\n",
    "        x_observed = x['observed'].to(device).type(torch.bool)\n",
    "        \n",
    "        if print_debug and i % progress_interval == 0:\n",
    "            print(f\"Step {i}; avg. loss {epoch_loss/(i+1)}\", end='\\r')\n",
    "            # p = pyro.param('p').detach().cpu()\n",
    "            # phi = torch.softmax(p, axis=-1)\n",
    "            # p_2d = trans.transform(p)\n",
    "            # latest_plotted_p[:] = p\n",
    "            # umap.plot.points(trans, labels=pwd.val.category,theme='fire')\n",
    "            # for ix, marker in enumerate(['$1$', '$2$', '$3$', '$4$', '$5$', '$6$', '$7$']):\n",
    "            #     plt.scatter(x=p_2d[ix,0], y=p_2d[ix,1], s=7**3, c='white', marker=marker)\n",
    "            # plt.title(f\"Frozen Enc/Dec : Epoch {epoch}, Step {i}\")\n",
    "            # plt.savefig(f'../data/figures_pretrained_unfrozen/epoch{epoch}_step{i}')\n",
    "            # plt.close()\n",
    "        \n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x_a, x_b, x_label, x_observed)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = svi.guide(x_a, x_b, x_label, x_observed)\n",
    "\n",
    "        x_label = x_label.cpu()\n",
    "        logits = x.cpu()\n",
    "        logits_epoch += logits\n",
    "        labels_epoch += x_label\n",
    "        \n",
    "    logits_binned = np.digitize(logits_epoch, [0.45], right=False)\n",
    "    train_acc = accuracy_score(labels_epoch, logits_binned)\n",
    "    train_auroc = roc_auc_score(labels_epoch, logits_epoch)\n",
    "    \n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train, train_acc, train_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "56ugiYHHyjuf"
   },
   "outputs": [],
   "source": [
    "def evaluate(svi, etm, test_loader, device): \n",
    "    etm.eval()\n",
    "    \n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.0\n",
    "    test_ce_loss = 0.0\n",
    "    test_kl_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_auroc = 0.0\n",
    "    batches = 0\n",
    "    logits_epoch = []\n",
    "    labels_epoch = []\n",
    "    \n",
    "    # compute the loss over the entire test set\n",
    "    for x in test_loader:\n",
    "        batches += 1\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        x_a = x['a'].to(device).squeeze()\n",
    "        x_b = x['b'].to(device).squeeze()\n",
    "        x_label = x['label'].to(device).type(torch.float32)\n",
    "        x_observed = x['observed'].to(device).type(torch.bool)\n",
    "            \n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x_a, x_b, x_label, x_observed)\n",
    "        \n",
    "        # generate reconstruction of batch documents and move to cuda if designated\n",
    "        with torch.no_grad():\n",
    "            recon_x_a = etm.reconstruct_doc(x_a)\n",
    "            recon_x_b = etm.reconstruct_doc(x_b)\n",
    "            logits = svi.guide(x_a, x_b, x_label, x_observed)\n",
    "\n",
    "        x_label = x_label.cpu()\n",
    "        logits = logits.cpu()\n",
    "        logits_epoch += logits\n",
    "        labels_epoch += x_label\n",
    "\n",
    "        # calculate and sum cross entropy loss and kl divergence\n",
    "        x_a=x_a.squeeze()\n",
    "        x_b=x_b.squeeze()\n",
    "        log_probs_a = torch.log(recon_x_a)\n",
    "        log_probs_b = torch.log(recon_x_b)\n",
    "        \n",
    "        if log_probs_a.isnan().any() | log_probs_b.isnan().any() | (log_probs_a.abs() > 30).any() | (log_probs_b.abs() > 30).any():\n",
    "            raise ValueError(\"nan or very large log probs\")\n",
    "            \n",
    "        targets_a = x_a/(x_a.sum(axis=-1)[:,None])\n",
    "        targets_b = x_b/(x_b.sum(axis=-1)[:,None])\n",
    "        ce_loss_a = F.cross_entropy(log_probs_a, targets_a, reduction='sum')\n",
    "        ce_loss_b = F.cross_entropy(log_probs_b, targets_b, reduction='sum')\n",
    "        test_ce_loss += ce_loss_a + ce_loss_b\n",
    "        test_kl_loss += etm.calc_kl_divergence(x_a.squeeze(1)) + etm.calc_kl_divergence(x_b.squeeze(1))\n",
    "\n",
    "        \n",
    "    logits_binned = np.digitize(logits_epoch, [0.45], right=False)\n",
    "    test_acc = accuracy_score(labels_epoch, logits_binned)\n",
    "    test_auroc = roc_auc_score(labels_epoch, logits_epoch)\n",
    "    \n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    epoch_elbo = test_loss / normalizer_test\n",
    "    epoch_ce_loss = test_ce_loss / normalizer_test\n",
    "    epoch_kl_loss = test_kl_loss / normalizer_test\n",
    "    \n",
    "    return epoch_elbo, epoch_ce_loss.item(), epoch_kl_loss.item(), test_acc, test_auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsaotgwIyjuf"
   },
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(pretrained, frozen, pwd):\n",
    "    try:\n",
    "        if frozen and not pretrained:\n",
    "            raise Exception\n",
    "    except:\n",
    "        print('ERROR: attempting to freeze an non-pretrained model!')\n",
    "        \n",
    "    # clear param store\n",
    "    pyro.clear_param_store()\n",
    "    # setup the VAE\n",
    "    topic_model = ProdLDA(pwd.vocab_size, NUM_TOPICS, NUM_PROTOTYPES, HIDDEN_DIM, DROPOUT_RATE, DEVICE, frozen=frozen).to(DEVICE)\n",
    "    \n",
    "    if pretrained:\n",
    "        saved_model_dict = torch.load(\"../data/pretrained_models/pretrained_20ng_noOF.pt\", map_location=DEVICE)\n",
    "        topic_model.load_state_dict(saved_model_dict['model'], strict=False)\n",
    "        # svi_guide = saved_model_dict['guide']\n",
    "        # pyro.get_param_store().load(\"../data/pretrained_models/pretrained_params_20ng_2000vocab.pt\")\n",
    "    \n",
    "    if frozen:\n",
    "        for param in topic_model.encoder.parameters():\n",
    "            param.requires_grad=False\n",
    "        for param in topic_model.decoder.parameters():\n",
    "            param.requires_grad=False\n",
    "        topic_model.frozen = True\n",
    "        \n",
    "    # setup the optimizer\n",
    "    optimizer = ClippedAdam(adam_args)\n",
    "    # setup the inference algorithm\n",
    "    svi = SVI(topic_model.model, topic_model.guide, optimizer, loss=custom_elbo)\n",
    "    \n",
    "\n",
    "    \n",
    "    return topic_model, svi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_model(topic_model, dl_train):\n",
    "    batch_1 = next(iter(dl_train))\n",
    "    dat_a = batch_1['a'].to(DEVICE).squeeze()\n",
    "    dat_b = batch_1['b'].to(DEVICE).squeeze()\n",
    "    dat_label = batch_1['label'].to(DEVICE).squeeze().type(torch.float32)\n",
    "    dat_observed = batch_1['observed'].to(DEVICE).squeeze().type(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map(topic_model):\n",
    "    x = torch.tensor(pwd.bows_val.toarray()).to(DEVICE).type(torch.float)\n",
    "    with torch.no_grad():\n",
    "        z_loc, z_scale = topic_model.encoder(x)\n",
    "\n",
    "    features = z_loc.cpu()\n",
    "    features = torch.softmax(features, axis=-1)\n",
    "    features = features.numpy()\n",
    "\n",
    "    trans = umap.UMAP(n_neighbors=15, random_state=42, min_dist=0.1).fit(features)\n",
    "    return trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vHKVjjOCyjug",
    "outputId": "ea1176e6-2e15-4934-f8ff-d1bfb18a0b31",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training\n",
      "Training for percentage: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdsaad\u001b[0m (\u001b[33mwitw\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dsaad/ci_research/prod_LDA/code/wandb/run-20230410_110047-gzrc1xyw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/witw/prodLDA_pairs/runs/gzrc1xyw' target=\"_blank\">daily-eon-518</a></strong> to <a href='https://wandb.ai/witw/prodLDA_pairs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/witw/prodLDA_pairs' target=\"_blank\">https://wandb.ai/witw/prodLDA_pairs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/witw/prodLDA_pairs/runs/gzrc1xyw' target=\"_blank\">https://wandb.ai/witw/prodLDA_pairs/runs/gzrc1xyw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 000]  average training loss: 495.9047\n",
      "\n",
      "Evaluation: \n",
      "[epoch 000]  average elbo loss: 484.9500\n",
      "              average ce loss:   14.6285\n",
      "              average kld loss:  53.7094\n",
      "              average accuracy:     0.5472\n",
      "              average auroc:  0.6091\n",
      "\n",
      "[epoch 001]  average training loss: 485.3542\n",
      "\n",
      "Evaluation: \n",
      "[epoch 001]  average elbo loss: 480.3098\n",
      "              average ce loss:   14.6219\n",
      "              average kld loss:  55.0881\n",
      "              average accuracy:     0.5518\n",
      "              average auroc:  0.6143\n",
      "\n",
      "[epoch 002]  average training loss: 479.6507\n",
      "\n",
      "Evaluation: \n",
      "[epoch 002]  average elbo loss: 482.9033\n",
      "              average ce loss:   14.5618\n",
      "              average kld loss:  63.8692\n",
      "              average accuracy:     0.5637\n",
      "              average auroc:  0.6213\n",
      "\n",
      "[epoch 003]  average training loss: 473.8896\n",
      "\n",
      "Evaluation: \n",
      "[epoch 003]  average elbo loss: 464.3277\n",
      "              average ce loss:   14.4605\n",
      "              average kld loss:  57.6087\n",
      "              average accuracy:     0.5670\n",
      "              average auroc:  0.6162\n",
      "\n",
      "[epoch 004]  average training loss: 469.6249\n",
      "\n",
      "Evaluation: \n",
      "[epoch 004]  average elbo loss: 465.9063\n",
      "              average ce loss:   14.4768\n",
      "              average kld loss:  60.3925\n",
      "              average accuracy:     0.5920\n",
      "              average auroc:  0.6210\n",
      "\n",
      "[epoch 005]  average training loss: 465.8444\n",
      "\n",
      "Evaluation: \n",
      "[epoch 005]  average elbo loss: 463.0639\n",
      "              average ce loss:   14.5115\n",
      "              average kld loss:  55.4133\n",
      "              average accuracy:     0.6405\n",
      "              average auroc:  0.6275\n",
      "\n",
      "[epoch 006]  average training loss: 463.8144\n",
      "\n",
      "Evaluation: \n",
      "[epoch 006]  average elbo loss: 463.4899\n",
      "              average ce loss:   14.4299\n",
      "              average kld loss:  61.4498\n",
      "              average accuracy:     0.7343\n",
      "              average auroc:  0.6290\n",
      "\n",
      "[epoch 007]  average training loss: 460.4297\n",
      "\n",
      "Evaluation: \n",
      "[epoch 007]  average elbo loss: 457.7698\n",
      "              average ce loss:   14.4843\n",
      "              average kld loss:  54.6393\n",
      "              average accuracy:     0.7961\n",
      "              average auroc:  0.6296\n",
      "\n",
      "[epoch 008]  average training loss: 458.6848\n",
      "\n",
      "Evaluation: \n",
      "[epoch 008]  average elbo loss: 463.0289\n",
      "              average ce loss:   14.4228\n",
      "              average kld loss:  63.2621\n",
      "              average accuracy:     0.7964\n",
      "              average auroc:  0.6340\n",
      "\n",
      "[epoch 009]  average training loss: 457.1841\n",
      "\n",
      "Evaluation: \n",
      "[epoch 009]  average elbo loss: 457.3483\n",
      "              average ce loss:   14.4502\n",
      "              average kld loss:  59.4132\n",
      "              average accuracy:     0.8002\n",
      "              average auroc:  0.6367\n",
      "\n",
      "[epoch 010]  average training loss: 456.6661\n",
      "\n",
      "Evaluation: \n",
      "[epoch 010]  average elbo loss: 454.9522\n",
      "              average ce loss:   14.4435\n",
      "              average kld loss:  56.4859\n",
      "              average accuracy:     0.8082\n",
      "              average auroc:  0.6420\n",
      "\n",
      "[epoch 011]  average training loss: 454.8405\n",
      "\n",
      "Evaluation: \n",
      "[epoch 011]  average elbo loss: 449.9536\n",
      "              average ce loss:   14.4279\n",
      "              average kld loss:  55.8425\n",
      "              average accuracy:     0.8066\n",
      "              average auroc:  0.6441\n",
      "\n",
      "[epoch 012]  average training loss: 453.7152\n",
      "\n",
      "Evaluation: \n",
      "[epoch 012]  average elbo loss: 454.0279\n",
      "              average ce loss:   14.4505\n",
      "              average kld loss:  54.0683\n",
      "              average accuracy:     0.8085\n",
      "              average auroc:  0.6452\n",
      "\n",
      "[epoch 013]  average training loss: 453.3247\n",
      "\n",
      "Evaluation: \n",
      "[epoch 013]  average elbo loss: 453.9033\n",
      "              average ce loss:   14.4219\n",
      "              average kld loss:  58.9097\n",
      "              average accuracy:     0.8045\n",
      "              average auroc:  0.6411\n",
      "\n",
      "[epoch 014]  average training loss: 452.1957\n",
      "\n",
      "Evaluation: \n",
      "[epoch 014]  average elbo loss: 451.0016\n",
      "              average ce loss:   14.4564\n",
      "              average kld loss:  53.2453\n",
      "              average accuracy:     0.8053\n",
      "              average auroc:  0.6501\n",
      "\n",
      "[epoch 015]  average training loss: 451.9179\n",
      "\n",
      "Evaluation: \n",
      "[epoch 015]  average elbo loss: 450.4729\n",
      "              average ce loss:   14.4070\n",
      "              average kld loss:  58.8721\n",
      "              average accuracy:     0.8025\n",
      "              average auroc:  0.6473\n",
      "\n",
      "[epoch 016]  average training loss: 450.8639\n",
      "\n",
      "Evaluation: \n",
      "[epoch 016]  average elbo loss: 452.0847\n",
      "              average ce loss:   14.3589\n",
      "              average kld loss:  57.8644\n",
      "              average accuracy:     0.8075\n",
      "              average auroc:  0.6601\n",
      "\n",
      "[epoch 017]  average training loss: 450.7070\n",
      "\n",
      "Evaluation: \n",
      "[epoch 017]  average elbo loss: 450.0718\n",
      "              average ce loss:   14.3941\n",
      "              average kld loss:  56.0149\n",
      "              average accuracy:     0.8097\n",
      "              average auroc:  0.6625\n",
      "\n",
      "[epoch 018]  average training loss: 450.5901\n",
      "\n",
      "Evaluation: \n",
      "[epoch 018]  average elbo loss: 446.1877\n",
      "              average ce loss:   14.3797\n",
      "              average kld loss:  56.5436\n",
      "              average accuracy:     0.8087\n",
      "              average auroc:  0.6557\n",
      "\n",
      "[epoch 019]  average training loss: 449.9163\n",
      "\n",
      "Evaluation: \n",
      "[epoch 019]  average elbo loss: 445.5453\n",
      "              average ce loss:   14.3542\n",
      "              average kld loss:  56.0012\n",
      "              average accuracy:     0.8103\n",
      "              average auroc:  0.6631\n",
      "\n",
      "[epoch 020]  average training loss: 449.8940\n",
      "\n",
      "Evaluation: \n",
      "[epoch 020]  average elbo loss: 452.0456\n",
      "              average ce loss:   14.3602\n",
      "              average kld loss:  60.4090\n",
      "              average accuracy:     0.8087\n",
      "              average auroc:  0.6676\n",
      "\n",
      "[epoch 021]  average training loss: 449.8498\n",
      "\n",
      "Evaluation: \n",
      "[epoch 021]  average elbo loss: 448.6224\n",
      "              average ce loss:   14.4567\n",
      "              average kld loss:  57.4860\n",
      "              average accuracy:     0.8066\n",
      "              average auroc:  0.6712\n",
      "\n",
      "[epoch 022]  average training loss: 448.9940\n",
      "\n",
      "Evaluation: \n",
      "[epoch 022]  average elbo loss: 445.6745\n",
      "              average ce loss:   14.3305\n",
      "              average kld loss:  58.4772\n",
      "              average accuracy:     0.8091\n",
      "              average auroc:  0.6740\n",
      "\n",
      "[epoch 023]  average training loss: 449.3526\n",
      "\n",
      "Evaluation: \n",
      "[epoch 023]  average elbo loss: 448.2442\n",
      "              average ce loss:   14.3970\n",
      "              average kld loss:  59.0118\n",
      "              average accuracy:     0.8113\n",
      "              average auroc:  0.6824\n",
      "\n",
      "[epoch 024]  average training loss: 448.6127\n",
      "\n",
      "Evaluation: \n",
      "[epoch 024]  average elbo loss: 446.4153\n",
      "              average ce loss:   14.2568\n",
      "              average kld loss:  62.4020\n",
      "              average accuracy:     0.8111\n",
      "              average auroc:  0.6784\n",
      "\n",
      "[epoch 025]  average training loss: 448.4834\n",
      "\n",
      "Evaluation: \n",
      "[epoch 025]  average elbo loss: 442.9265\n",
      "              average ce loss:   14.3598\n",
      "              average kld loss:  54.4625\n",
      "              average accuracy:     0.8109\n",
      "              average auroc:  0.6813\n",
      "\n",
      "[epoch 026]  average training loss: 448.2879\n",
      "\n",
      "Evaluation: \n",
      "[epoch 026]  average elbo loss: 446.9841\n",
      "              average ce loss:   14.3948\n",
      "              average kld loss:  58.6187\n",
      "              average accuracy:     0.8059\n",
      "              average auroc:  0.6755\n",
      "\n",
      "[epoch 027]  average training loss: 448.4102\n",
      "\n",
      "Evaluation: \n",
      "[epoch 027]  average elbo loss: 448.7244\n",
      "              average ce loss:   14.4011\n",
      "              average kld loss:  60.5378\n",
      "              average accuracy:     0.8073\n",
      "              average auroc:  0.6794\n",
      "\n",
      "[epoch 028]  average training loss: 448.0310\n",
      "\n",
      "Evaluation: \n",
      "[epoch 028]  average elbo loss: 444.0260\n",
      "              average ce loss:   14.3068\n",
      "              average kld loss:  57.7587\n",
      "              average accuracy:     0.8131\n",
      "              average auroc:  0.6855\n",
      "\n",
      "[epoch 029]  average training loss: 447.9796\n",
      "\n",
      "Evaluation: \n",
      "[epoch 029]  average elbo loss: 448.7711\n",
      "              average ce loss:   14.3012\n",
      "              average kld loss:  60.9630\n",
      "              average accuracy:     0.8102\n",
      "              average auroc:  0.6830\n",
      "\n",
      "[epoch 030]  average training loss: 447.6778\n",
      "\n",
      "Evaluation: \n",
      "[epoch 030]  average elbo loss: 444.4319\n",
      "              average ce loss:   14.3204\n",
      "              average kld loss:  56.9378\n",
      "              average accuracy:     0.8128\n",
      "              average auroc:  0.6922\n",
      "\n",
      "[epoch 031]  average training loss: 446.7631\n",
      "\n",
      "Evaluation: \n",
      "[epoch 031]  average elbo loss: 447.1419\n",
      "              average ce loss:   14.3618\n",
      "              average kld loss:  60.2445\n",
      "              average accuracy:     0.8134\n",
      "              average auroc:  0.6894\n",
      "\n",
      "[epoch 032]  average training loss: 447.8076\n",
      "\n",
      "Evaluation: \n",
      "[epoch 032]  average elbo loss: 444.6626\n",
      "              average ce loss:   14.3739\n",
      "              average kld loss:  56.5888\n",
      "              average accuracy:     0.8145\n",
      "              average auroc:  0.6943\n",
      "\n",
      "[epoch 033]  average training loss: 446.9330\n",
      "\n",
      "Evaluation: \n",
      "[epoch 033]  average elbo loss: 442.6936\n",
      "              average ce loss:   14.4295\n",
      "              average kld loss:  53.5509\n",
      "              average accuracy:     0.8141\n",
      "              average auroc:  0.6932\n",
      "\n",
      "[epoch 034]  average training loss: 447.6829\n",
      "\n",
      "Evaluation: \n",
      "[epoch 034]  average elbo loss: 444.5990\n",
      "              average ce loss:   14.3428\n",
      "              average kld loss:  58.0532\n",
      "              average accuracy:     0.8100\n",
      "              average auroc:  0.6882\n",
      "\n",
      "[epoch 035]  average training loss: 446.9656\n",
      "\n",
      "Evaluation: \n",
      "[epoch 035]  average elbo loss: 446.3093\n",
      "              average ce loss:   14.3116\n",
      "              average kld loss:  61.4841\n",
      "              average accuracy:     0.8126\n",
      "              average auroc:  0.6933\n",
      "\n",
      "[epoch 036]  average training loss: 447.2445\n",
      "\n",
      "Evaluation: \n",
      "[epoch 036]  average elbo loss: 443.3857\n",
      "              average ce loss:   14.4490\n",
      "              average kld loss:  52.4189\n",
      "              average accuracy:     0.8130\n",
      "              average auroc:  0.6871\n",
      "\n",
      "[epoch 037]  average training loss: 447.0338\n",
      "\n",
      "Evaluation: \n",
      "[epoch 037]  average elbo loss: 443.4531\n",
      "              average ce loss:   14.3218\n",
      "              average kld loss:  59.5142\n",
      "              average accuracy:     0.8135\n",
      "              average auroc:  0.6925\n",
      "\n",
      "[epoch 038]  average training loss: 447.7217\n",
      "\n",
      "Evaluation: \n",
      "[epoch 038]  average elbo loss: 443.9049\n",
      "              average ce loss:   14.4698\n",
      "              average kld loss:  54.1944\n",
      "              average accuracy:     0.8131\n",
      "              average auroc:  0.6912\n",
      "\n",
      "[epoch 039]  average training loss: 447.1604\n",
      "\n",
      "Evaluation: \n",
      "[epoch 039]  average elbo loss: 441.7005\n",
      "              average ce loss:   14.3799\n",
      "              average kld loss:  54.4340\n",
      "              average accuracy:     0.8141\n",
      "              average auroc:  0.6960\n",
      "\n",
      "[epoch 040]  average training loss: 446.8981\n",
      "\n",
      "Evaluation: \n",
      "[epoch 040]  average elbo loss: 443.2851\n",
      "              average ce loss:   14.4105\n",
      "              average kld loss:  55.3157\n",
      "              average accuracy:     0.8142\n",
      "              average auroc:  0.7028\n",
      "\n",
      "[epoch 041]  average training loss: 446.6623\n",
      "\n",
      "Evaluation: \n",
      "[epoch 041]  average elbo loss: 441.1549\n",
      "              average ce loss:   14.3848\n",
      "              average kld loss:  51.9675\n",
      "              average accuracy:     0.8166\n",
      "              average auroc:  0.7094\n",
      "\n",
      "[epoch 042]  average training loss: 447.2212\n",
      "\n",
      "Evaluation: \n",
      "[epoch 042]  average elbo loss: 441.0728\n",
      "              average ce loss:   14.3891\n",
      "              average kld loss:  52.4986\n",
      "              average accuracy:     0.8152\n",
      "              average auroc:  0.7043\n",
      "\n",
      "[epoch 043]  average training loss: 446.6521\n",
      "\n",
      "Evaluation: \n",
      "[epoch 043]  average elbo loss: 446.7743\n",
      "              average ce loss:   14.3778\n",
      "              average kld loss:  59.2545\n",
      "              average accuracy:     0.8152\n",
      "              average auroc:  0.7080\n",
      "\n",
      "[epoch 044]  average training loss: 446.3947\n",
      "\n",
      "Evaluation: \n",
      "[epoch 044]  average elbo loss: 442.7916\n",
      "              average ce loss:   14.4103\n",
      "              average kld loss:  53.5696\n",
      "              average accuracy:     0.8162\n",
      "              average auroc:  0.7124\n",
      "\n",
      "[epoch 045]  average training loss: 446.5313\n",
      "\n",
      "Evaluation: \n",
      "[epoch 045]  average elbo loss: 443.8602\n",
      "              average ce loss:   14.3170\n",
      "              average kld loss:  57.2165\n",
      "              average accuracy:     0.8165\n",
      "              average auroc:  0.7198\n",
      "\n",
      "[epoch 046]  average training loss: 446.1221\n",
      "\n",
      "Evaluation: \n",
      "[epoch 046]  average elbo loss: 441.4311\n",
      "              average ce loss:   14.4162\n",
      "              average kld loss:  52.8305\n",
      "              average accuracy:     0.8163\n",
      "              average auroc:  0.7237\n",
      "\n",
      "[epoch 047]  average training loss: 446.4627\n",
      "\n",
      "Evaluation: \n",
      "[epoch 047]  average elbo loss: 445.2295\n",
      "              average ce loss:   14.4380\n",
      "              average kld loss:  55.8965\n",
      "              average accuracy:     0.8175\n",
      "              average auroc:  0.7251\n",
      "\n",
      "[epoch 048]  average training loss: 445.8647\n",
      "\n",
      "Evaluation: \n",
      "[epoch 048]  average elbo loss: 447.6782\n",
      "              average ce loss:   14.3382\n",
      "              average kld loss:  61.4003\n",
      "              average accuracy:     0.8157\n",
      "              average auroc:  0.7198\n",
      "\n",
      "[epoch 049]  average training loss: 446.3258\n",
      "\n",
      "Evaluation: \n",
      "[epoch 049]  average elbo loss: 445.0241\n",
      "              average ce loss:   14.3387\n",
      "              average kld loss:  58.0475\n",
      "              average accuracy:     0.8169\n",
      "              average auroc:  0.7310\n",
      "\n",
      "[epoch 050]  average training loss: 446.0927\n",
      "\n",
      "Evaluation: \n",
      "[epoch 050]  average elbo loss: 444.6371\n",
      "              average ce loss:   14.3475\n",
      "              average kld loss:  56.8495\n",
      "              average accuracy:     0.8166\n",
      "              average auroc:  0.7272\n",
      "\n",
      "[epoch 051]  average training loss: 445.8952\n",
      "\n",
      "Evaluation: \n",
      "[epoch 051]  average elbo loss: 445.0773\n",
      "              average ce loss:   14.3638\n",
      "              average kld loss:  56.7062\n",
      "              average accuracy:     0.8147\n",
      "              average auroc:  0.7214\n",
      "\n",
      "[epoch 052]  average training loss: 446.3250\n",
      "\n",
      "Evaluation: \n",
      "[epoch 052]  average elbo loss: 445.3599\n",
      "              average ce loss:   14.2745\n",
      "              average kld loss:  59.9439\n",
      "              average accuracy:     0.8167\n",
      "              average auroc:  0.7295\n",
      "\n",
      "[epoch 053]  average training loss: 446.1498\n",
      "\n",
      "Evaluation: \n",
      "[epoch 053]  average elbo loss: 436.7210\n",
      "              average ce loss:   14.3962\n",
      "              average kld loss:  50.4223\n",
      "              average accuracy:     0.8157\n",
      "              average auroc:  0.7285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging = True\n",
    "pretrained = True\n",
    "frozen = False\n",
    "run_id = \"s23_noOF_logtheta10_simscale10_5prototypes_lr0003\"\n",
    "\n",
    "run_name_original = (\"pretrained_\" if pretrained else \"unpretrained_\") + (\"frozen_\" if frozen else \"unfrozen_\") + \"20ng_\" + run_id\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "test_celoss = []\n",
    "test_klloss = []\n",
    "\n",
    "print(\"Beginning Training\")\n",
    "\n",
    "# training loop\n",
    "\n",
    "for p_percentage in [.01]:\n",
    "    pyro.set_rng_seed(0)\n",
    "    run_name = run_name_original + f\"_lp{int(p_percentage * 100)}\"\n",
    "    print(f\"Training for percentage: {p_percentage}\")\n",
    "    best_accuracy = 0\n",
    "    pwd, dl_train, dl_val, dl_test = get_data(p_percentage)\n",
    "    topic_model, svi = init_model(pretrained=pretrained, frozen=frozen, pwd=pwd)\n",
    "    if logging:\n",
    "        run = wandb.init(project=\"prodLDA_pairs\", entity=\"witw\", )\n",
    "        wandb.run.name = run_name \n",
    "        wandb.watch(topic_model, log='all', log_freq=100)\n",
    "    trace_model(topic_model, dl_train=dl_train)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        trans = get_map(topic_model)\n",
    "        total_epoch_loss_train, _, _ = train(svi, topic_model, dl_train, device=DEVICE, print_debug=True, epoch=epoch, latest_plotted_p=latest_plotted_p, trans = trans)\n",
    "        train_elbo.append(-total_epoch_loss_train)\n",
    "\n",
    "        if logging:\n",
    "            wandb.log({'epoch': epoch,\n",
    "                       'train_elbo': total_epoch_loss_train})\n",
    "\n",
    "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "        if epoch % TEST_FREQUENCY == 0:\n",
    "            # report test diagnostics\n",
    "            total_epoch_loss_test, total_epoch_celoss_test, total_epoch_klloss_test, test_acc, test_auroc = evaluate(svi, topic_model, dl_val, device=DEVICE)\n",
    "            test_elbo.append(-total_epoch_loss_test)\n",
    "            test_celoss.append(total_epoch_celoss_test)\n",
    "            test_klloss.append(total_epoch_klloss_test)\n",
    "    #         print(x)\n",
    "            if logging:\n",
    "                wandb.log({'epoch': epoch,\n",
    "                           'test_acc': test_acc,\n",
    "                           'test_auroc': test_auroc,\n",
    "                           'test_elbo': total_epoch_loss_test,\n",
    "                           'test_entropy': total_epoch_celoss_test,\n",
    "                           'test_kl': total_epoch_klloss_test})\n",
    "\n",
    "            print(\"\\nEvaluation: \")\n",
    "            print(\"[epoch %03d]  average elbo loss: %.4f\" % (epoch, total_epoch_loss_test))\n",
    "            print(\"              average ce loss:   %.4f\" % (total_epoch_celoss_test))\n",
    "            print(\"              average kld loss:  %.4f\" % (total_epoch_klloss_test))\n",
    "            print(\"              average accuracy:     %.4f\" % (test_acc))\n",
    "            print(\"              average auroc:  %.4f\\n\" % (test_auroc))\n",
    "            if test_acc > best_accuracy:\n",
    "                best_accuracy = test_acc\n",
    "                torch.save({\"model\" : topic_model.state_dict()}, f\"../data/20ng_checkpoints/labeled_percentage_sweep/bestmodel_\" + run_name + \".pt\")\n",
    "    torch.save({\"model\" : topic_model.state_dict()},  f\"../data/20ng_checkpoints/labeled_percentage_sweep/lastmodel_\" + run_name + \".pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2ix = pwd.vectorizer.vocabulary_\n",
    "ix2tok = {v:k for k,v in tok2ix.items()}\n",
    "with torch.no_grad():\n",
    "    phi_decoded = topic_model.decoder(phi.to(DEVICE))\n",
    "top_term_ixs = phi_decoded.argsort(axis=-1, descending=True)[:,:15]\n",
    "\n",
    "topic_terms = []\n",
    "for jx, topic in enumerate(top_term_ixs):\n",
    "    # print(f\"THEME {jx+1}\")\n",
    "    terms = [ix2tok[ix.item()] for ix in topic]\n",
    "    # for term in terms:\n",
    "        # print(term)\n",
    "    topic_terms.append(terms)\n",
    "    # print()\n",
    "    \n",
    "df = pd.DataFrame(topic_terms)\n",
    "df = df.transpose()\n",
    "df.columns = [\"Theme A\", \"Theme B\", \"Theme C\", \"Theme D\", \"Theme E\", \"Theme F\", \"Theme G\"]\n",
    "df.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ProdLDA_semi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
