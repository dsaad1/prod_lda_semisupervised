{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqhPcS1AyjuX"
   },
   "source": [
    "### Installs, Imports, and Pyro Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EkkTbMWyjuZ",
    "outputId": "e1cb369f-b60c-45a5-9a9f-c196a08adad5"
   },
   "outputs": [],
   "source": [
    "# 1# Run first time\n",
    "# ! module load cuda/9.2.88-gcc/7.1.0 cudnn/7.6.5.32-9.2-linux-x64-gcc/7.1.0-cuda9_2 anaconda3/2019.10-gcc/8.3.1\n",
    "# ! source activate pytorch_env\n",
    "# !pip install --up\n",
    "# !pip install pyro-ppl\n",
    "# !pip install torchvision\n",
    "# !pip install --upgrade git+https://github.com/dhudsmith/clean-the-text\n",
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W39rR1nNyjuZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsaad/.conda/envs/updated_pytorch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from imdb_dataset import PairwiseData, DocumentPairData\n",
    "from model import Encoder, Decoder, ProdLDA, custom_elbo\n",
    "import os\n",
    "import pickle as pkl\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "import pyro\n",
    "from pyro import poutine\n",
    "import pyro.distributions as dist\n",
    "# import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "import torch.nn.functional as F\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import umap\n",
    "import umap.plot\n",
    "import sklearn.datasets\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LpuHjrWDyjua"
   },
   "outputs": [],
   "source": [
    "pyro.distributions.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1-TcWDrjyjua"
   },
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-Gm9gInyjua",
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fWiuASbwyjua"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "NUMBER_PAIRS = 100000\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Dropout rate\n",
    "NUM_TOPICS = 50\n",
    "NUM_PROTOTYPES = 5\n",
    "EMBED_DIM  = 64\n",
    "HIDDEN_DIM = 128\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# Training\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS = 75\n",
    "TEST_FREQUENCY = 1\n",
    "BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 2000\n",
    "OBS_SAMPLES = 10\n",
    "\n",
    "# gamma = 0.25\n",
    "# num_steps = (NUMBER_PAIRS // BATCH_SIZE) * NUM_EPOCHS\n",
    "# lrd = np.exp(np.log(gamma)/num_steps)\n",
    "adam_args = {\"lr\": 0.003, 'clip_norm':10.0, 'betas': (0.99, 0.999)}\n",
    "\n",
    "latest_plotted_p = torch.zeros(NUM_PROTOTYPES, NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhrFMmkEyjub",
    "tags": []
   },
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oHW-3swTyjue"
   },
   "outputs": [],
   "source": [
    "def get_data(pair_percentage=1.0):\n",
    "    # pairwise data\n",
    "    pwd = PairwiseData()\n",
    "    train_pairs, val_pairs, test_pairs = [pwd.get_pairs_table(d) for d in [pwd.train, pwd.val, pwd.test]]\n",
    "\n",
    "    # datasets\n",
    "    data_train, data_val, data_test = [\n",
    "        DocumentPairData(bows=pwd.bows, index_table=ix_table, prob=pair_percentage)\n",
    "        for ix_table in [train_pairs, val_pairs, test_pairs]\n",
    "    ]\n",
    "\n",
    "    # dataloaders\n",
    "    dl_train = DataLoader(data_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "    dl_val = DataLoader(data_val, batch_size=VAL_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "    dl_test = DataLoader(data_test, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "    \n",
    "    return pwd, dl_train, dl_val, dl_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fk2anUsCyjuf"
   },
   "source": [
    "# Training and Evaluation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "q-wenFNRyjuf"
   },
   "outputs": [],
   "source": [
    "def train(svi, topic_model, train_loader, device, progress_interval = 100, print_debug = False, epoch = 0, latest_plotted_p = None, trans = None):\n",
    "    topic_model.train()\n",
    "    \n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    logits_epoch = []\n",
    "    labels_epoch = []\n",
    "    batches = 0\n",
    "    \n",
    "    \n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for i, x in enumerate(train_loader):\n",
    "        batches += 1\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        x_a = x['a'].to(device).squeeze()\n",
    "        x_b = x['b'].to(device).squeeze()\n",
    "        x_label = x['label'].to(device).type(torch.float32)\n",
    "        x_observed = x['observed'].to(device).type(torch.bool)\n",
    "        \n",
    "        if print_debug and i % progress_interval == 0:\n",
    "            print(f\"Step {i}; avg. loss {epoch_loss/(i+1)}\", end='\\r')\n",
    "            # p = pyro.param('p').detach().cpu()\n",
    "            # phi = torch.softmax(p, axis=-1)\n",
    "            # p_2d = trans.transform(p)\n",
    "            # latest_plotted_p[:] = p\n",
    "            # umap.plot.points(trans, labels=pwd.val.category,theme='fire')\n",
    "            # for ix, marker in enumerate(['$1$', '$2$', '$3$', '$4$', '$5$', '$6$', '$7$']):\n",
    "            #     plt.scatter(x=p_2d[ix,0], y=p_2d[ix,1], s=7**3, c='white', marker=marker)\n",
    "            # plt.title(f\"Frozen Enc/Dec : Epoch {epoch}, Step {i}\")\n",
    "            # plt.savefig(f'../data/figures_pretrained_unfrozen/epoch{epoch}_step{i}')\n",
    "            # plt.close()\n",
    "        \n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x_a, x_b, x_label, x_observed)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = svi.guide(x_a, x_b, x_label, x_observed)\n",
    "\n",
    "        x_label = x_label.cpu()\n",
    "        logits = x.cpu()\n",
    "        logits_epoch += logits\n",
    "        labels_epoch += x_label\n",
    "        \n",
    "    logits_binned = np.digitize(logits_epoch, [0.45], right=False)\n",
    "    train_acc = accuracy_score(labels_epoch, logits_binned)\n",
    "    train_auroc = roc_auc_score(labels_epoch, logits_epoch)\n",
    "    \n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train, train_acc, train_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "56ugiYHHyjuf"
   },
   "outputs": [],
   "source": [
    "def evaluate(svi, etm, test_loader, device): \n",
    "    etm.eval()\n",
    "    \n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.0\n",
    "    test_ce_loss = 0.0\n",
    "    test_kl_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_auroc = 0.0\n",
    "    batches = 0\n",
    "    logits_epoch = []\n",
    "    labels_epoch = []\n",
    "    \n",
    "    # compute the loss over the entire test set\n",
    "    for x in test_loader:\n",
    "        batches += 1\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        x_a = x['a'].to(device).squeeze()\n",
    "        x_b = x['b'].to(device).squeeze()\n",
    "        x_label = x['label'].to(device).type(torch.float32)\n",
    "        x_observed = x['observed'].to(device).type(torch.bool)\n",
    "            \n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x_a, x_b, x_label, x_observed)\n",
    "        \n",
    "        # generate reconstruction of batch documents and move to cuda if designated\n",
    "        with torch.no_grad():\n",
    "            recon_x_a = etm.reconstruct_doc(x_a)\n",
    "            recon_x_b = etm.reconstruct_doc(x_b)\n",
    "            logits = svi.guide(x_a, x_b, x_label, x_observed)\n",
    "\n",
    "        x_label = x_label.cpu()\n",
    "        logits = logits.cpu()\n",
    "        logits_epoch += logits\n",
    "        labels_epoch += x_label\n",
    "\n",
    "        # calculate and sum cross entropy loss and kl divergence\n",
    "        x_a=x_a.squeeze()\n",
    "        x_b=x_b.squeeze()\n",
    "        log_probs_a = torch.log(recon_x_a)\n",
    "        log_probs_b = torch.log(recon_x_b)\n",
    "        \n",
    "        if log_probs_a.isnan().any() | log_probs_b.isnan().any() | (log_probs_a.abs() > 30).any() | (log_probs_b.abs() > 30).any():\n",
    "            raise ValueError(\"nan or very large log probs\")\n",
    "            \n",
    "        targets_a = x_a/(x_a.sum(axis=-1)[:,None])\n",
    "        targets_b = x_b/(x_b.sum(axis=-1)[:,None])\n",
    "        ce_loss_a = F.cross_entropy(log_probs_a, targets_a, reduction='sum')\n",
    "        ce_loss_b = F.cross_entropy(log_probs_b, targets_b, reduction='sum')\n",
    "        test_ce_loss += ce_loss_a + ce_loss_b\n",
    "        test_kl_loss += etm.calc_kl_divergence(x_a.squeeze(1)) + etm.calc_kl_divergence(x_b.squeeze(1))\n",
    "\n",
    "        \n",
    "    logits_binned = np.digitize(logits_epoch, [0.45], right=False)\n",
    "    test_acc = accuracy_score(labels_epoch, logits_binned)\n",
    "    test_auroc = roc_auc_score(labels_epoch, logits_epoch)\n",
    "    \n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    epoch_elbo = test_loss / normalizer_test\n",
    "    epoch_ce_loss = test_ce_loss / normalizer_test\n",
    "    epoch_kl_loss = test_kl_loss / normalizer_test\n",
    "    \n",
    "    return epoch_elbo, epoch_ce_loss.item(), epoch_kl_loss.item(), test_acc, test_auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsaotgwIyjuf"
   },
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(pretrained, frozen, pwd):\n",
    "    try:\n",
    "        if frozen and not pretrained:\n",
    "            raise Exception\n",
    "    except:\n",
    "        print('ERROR: attempting to freeze an non-pretrained model!')\n",
    "        \n",
    "    # clear param store\n",
    "    pyro.clear_param_store()\n",
    "    # setup the VAE\n",
    "    topic_model = ProdLDA(pwd.vocab_size, NUM_TOPICS, NUM_PROTOTYPES, HIDDEN_DIM, DROPOUT_RATE, DEVICE, frozen=frozen).to(DEVICE)\n",
    "    \n",
    "    if pretrained:\n",
    "        saved_model_dict = torch.load(\"../data/pretrained_models/pretrained_movies_noOF.pt\", map_location=DEVICE)\n",
    "        topic_model.load_state_dict(saved_model_dict['model'], strict=False)\n",
    "    \n",
    "    if frozen:\n",
    "        for param in topic_model.encoder.parameters():\n",
    "            param.requires_grad=False\n",
    "        for param in topic_model.decoder.parameters():\n",
    "            param.requires_grad=False\n",
    "        topic_model.frozen = True\n",
    "        \n",
    "    # setup the optimizer\n",
    "    optimizer = ClippedAdam(adam_args)\n",
    "    # setup the inference algorithm\n",
    "    svi = SVI(topic_model.model, topic_model.guide, optimizer, loss=custom_elbo)\n",
    "    \n",
    "\n",
    "    \n",
    "    return topic_model, svi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_model(topic_model, dl_train):\n",
    "    batch_1 = next(iter(dl_train))\n",
    "    dat_a = batch_1['a'].to(DEVICE).squeeze()\n",
    "    dat_b = batch_1['b'].to(DEVICE).squeeze()\n",
    "    dat_label = batch_1['label'].to(DEVICE).squeeze().type(torch.float32)\n",
    "    dat_observed = batch_1['observed'].to(DEVICE).squeeze().type(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map(topic_model):\n",
    "    x = torch.tensor(pwd.bows_val.toarray()).to(DEVICE).type(torch.float)\n",
    "    with torch.no_grad():\n",
    "        z_loc, z_scale = topic_model.encoder(x)\n",
    "\n",
    "    features = z_loc.cpu()\n",
    "    features = torch.softmax(features, axis=-1)\n",
    "    features = features.numpy()\n",
    "\n",
    "    trans = umap.UMAP(n_neighbors=15, random_state=42, min_dist=0.1).fit(features)\n",
    "    return trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vHKVjjOCyjug",
    "outputId": "ea1176e6-2e15-4934-f8ff-d1bfb18a0b31",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training\n",
      "Training for percentage: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdsaad\u001b[0m (\u001b[33mwitw\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dsaad/ci_research/prod_LDA/code/wandb/run-20230410_110037-ljuax0fq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/witw/prodLDA_pairs/runs/ljuax0fq' target=\"_blank\">summer-water-517</a></strong> to <a href='https://wandb.ai/witw/prodLDA_pairs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/witw/prodLDA_pairs' target=\"_blank\">https://wandb.ai/witw/prodLDA_pairs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/witw/prodLDA_pairs/runs/ljuax0fq' target=\"_blank\">https://wandb.ai/witw/prodLDA_pairs/runs/ljuax0fq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 000]  average training loss: 1067.8990\n",
      "\n",
      "Evaluation: \n",
      "[epoch 000]  average elbo loss: 1058.9885\n",
      "              average ce loss:   17.8221\n",
      "              average kld loss:  51.0072\n",
      "              average accuracy:     0.5457\n",
      "              average auroc:  0.5665\n",
      "\n",
      "[epoch 001]  average training loss: 1067.1124\n",
      "\n",
      "Evaluation: \n",
      "[epoch 001]  average elbo loss: 1058.0290\n",
      "              average ce loss:   17.7233\n",
      "              average kld loss:  55.8214\n",
      "              average accuracy:     0.5490\n",
      "              average auroc:  0.5741\n",
      "\n",
      "[epoch 002]  average training loss: 1066.5210\n",
      "\n",
      "Evaluation: \n",
      "[epoch 002]  average elbo loss: 1054.9666\n",
      "              average ce loss:   17.7525\n",
      "              average kld loss:  51.4474\n",
      "              average accuracy:     0.5521\n",
      "              average auroc:  0.5764\n",
      "\n",
      "[epoch 003]  average training loss: 1066.3741\n",
      "\n",
      "Evaluation: \n",
      "[epoch 003]  average elbo loss: 1050.5364\n",
      "              average ce loss:   17.6871\n",
      "              average kld loss:  49.0715\n",
      "              average accuracy:     0.5580\n",
      "              average auroc:  0.5800\n",
      "\n",
      "[epoch 004]  average training loss: 1066.0395\n",
      "\n",
      "Evaluation: \n",
      "[epoch 004]  average elbo loss: 1059.3416\n",
      "              average ce loss:   17.8117\n",
      "              average kld loss:  54.0305\n",
      "              average accuracy:     0.5615\n",
      "              average auroc:  0.5829\n",
      "\n",
      "[epoch 005]  average training loss: 1065.8196\n",
      "\n",
      "Evaluation: \n",
      "[epoch 005]  average elbo loss: 1058.0405\n",
      "              average ce loss:   17.8230\n",
      "              average kld loss:  51.6283\n",
      "              average accuracy:     0.5753\n",
      "              average auroc:  0.5872\n",
      "\n",
      "[epoch 006]  average training loss: 1065.5899\n",
      "\n",
      "Evaluation: \n",
      "[epoch 006]  average elbo loss: 1055.3735\n",
      "              average ce loss:   17.7362\n",
      "              average kld loss:  51.3021\n",
      "              average accuracy:     0.6017\n",
      "              average auroc:  0.5859\n",
      "\n",
      "[epoch 007]  average training loss: 1065.5356\n",
      "\n",
      "Evaluation: \n",
      "[epoch 007]  average elbo loss: 1067.0316\n",
      "              average ce loss:   17.9421\n",
      "              average kld loss:  50.0208\n",
      "              average accuracy:     0.6271\n",
      "              average auroc:  0.5945\n",
      "\n",
      "[epoch 008]  average training loss: 1065.6345\n",
      "\n",
      "Evaluation: \n",
      "[epoch 008]  average elbo loss: 1059.6233\n",
      "              average ce loss:   17.8037\n",
      "              average kld loss:  47.1941\n",
      "              average accuracy:     0.6156\n",
      "              average auroc:  0.5844\n",
      "\n",
      "[epoch 009]  average training loss: 1065.7637\n",
      "\n",
      "Evaluation: \n",
      "[epoch 009]  average elbo loss: 1055.7388\n",
      "              average ce loss:   17.7866\n",
      "              average kld loss:  47.3310\n",
      "              average accuracy:     0.6391\n",
      "              average auroc:  0.5911\n",
      "\n",
      "[epoch 010]  average training loss: 1065.5470\n",
      "\n",
      "Evaluation: \n",
      "[epoch 010]  average elbo loss: 1051.0727\n",
      "              average ce loss:   17.6956\n",
      "              average kld loss:  52.2734\n",
      "              average accuracy:     0.6273\n",
      "              average auroc:  0.5878\n",
      "\n",
      "[epoch 011]  average training loss: 1065.5168\n",
      "\n",
      "Evaluation: \n",
      "[epoch 011]  average elbo loss: 1059.7287\n",
      "              average ce loss:   17.8321\n",
      "              average kld loss:  46.9574\n",
      "              average accuracy:     0.6427\n",
      "              average auroc:  0.5961\n",
      "\n",
      "[epoch 012]  average training loss: 1065.3464\n",
      "\n",
      "Evaluation: \n",
      "[epoch 012]  average elbo loss: 1051.7082\n",
      "              average ce loss:   17.7203\n",
      "              average kld loss:  50.1015\n",
      "              average accuracy:     0.6355\n",
      "              average auroc:  0.5972\n",
      "\n",
      "[epoch 013]  average training loss: 1065.5734\n",
      "\n",
      "Evaluation: \n",
      "[epoch 013]  average elbo loss: 1057.2815\n",
      "              average ce loss:   17.7876\n",
      "              average kld loss:  48.7569\n",
      "              average accuracy:     0.6442\n",
      "              average auroc:  0.6052\n",
      "\n",
      "[epoch 014]  average training loss: 1065.5305\n",
      "\n",
      "Evaluation: \n",
      "[epoch 014]  average elbo loss: 1051.2200\n",
      "              average ce loss:   17.7368\n",
      "              average kld loss:  47.8935\n",
      "              average accuracy:     0.6395\n",
      "              average auroc:  0.6096\n",
      "\n",
      "[epoch 015]  average training loss: 1065.3314\n",
      "\n",
      "Evaluation: \n",
      "[epoch 015]  average elbo loss: 1052.0708\n",
      "              average ce loss:   17.7014\n",
      "              average kld loss:  50.5222\n",
      "              average accuracy:     0.6441\n",
      "              average auroc:  0.6175\n",
      "\n",
      "[epoch 016]  average training loss: 1065.4028\n",
      "\n",
      "Evaluation: \n",
      "[epoch 016]  average elbo loss: 1057.1142\n",
      "              average ce loss:   17.8059\n",
      "              average kld loss:  48.5214\n",
      "              average accuracy:     0.6481\n",
      "              average auroc:  0.6199\n",
      "\n",
      "[epoch 017]  average training loss: 1065.3896\n",
      "\n",
      "Evaluation: \n",
      "[epoch 017]  average elbo loss: 1058.4349\n",
      "              average ce loss:   17.8140\n",
      "              average kld loss:  46.3668\n",
      "              average accuracy:     0.6490\n",
      "              average auroc:  0.6243\n",
      "\n",
      "[epoch 018]  average training loss: 1065.3220\n",
      "\n",
      "Evaluation: \n",
      "[epoch 018]  average elbo loss: 1059.6640\n",
      "              average ce loss:   17.7741\n",
      "              average kld loss:  51.3351\n",
      "              average accuracy:     0.6492\n",
      "              average auroc:  0.6270\n",
      "\n",
      "[epoch 019]  average training loss: 1065.3942\n",
      "\n",
      "Evaluation: \n",
      "[epoch 019]  average elbo loss: 1067.1374\n",
      "              average ce loss:   17.9168\n",
      "              average kld loss:  47.9554\n",
      "              average accuracy:     0.6517\n",
      "              average auroc:  0.6397\n",
      "\n",
      "[epoch 020]  average training loss: 1065.5077\n",
      "\n",
      "Evaluation: \n",
      "[epoch 020]  average elbo loss: 1056.4124\n",
      "              average ce loss:   17.8303\n",
      "              average kld loss:  47.4589\n",
      "              average accuracy:     0.6573\n",
      "              average auroc:  0.6472\n",
      "\n",
      "[epoch 021]  average training loss: 1065.2480\n",
      "\n",
      "Evaluation: \n",
      "[epoch 021]  average elbo loss: 1062.0574\n",
      "              average ce loss:   17.8649\n",
      "              average kld loss:  50.6249\n",
      "              average accuracy:     0.6562\n",
      "              average auroc:  0.6532\n",
      "\n",
      "[epoch 022]  average training loss: 1065.3965\n",
      "\n",
      "Evaluation: \n",
      "[epoch 022]  average elbo loss: 1059.5024\n",
      "              average ce loss:   17.7841\n",
      "              average kld loss:  50.5355\n",
      "              average accuracy:     0.6602\n",
      "              average auroc:  0.6558\n",
      "\n",
      "[epoch 023]  average training loss: 1065.3964\n",
      "\n",
      "Evaluation: \n",
      "[epoch 023]  average elbo loss: 1052.3982\n",
      "              average ce loss:   17.7649\n",
      "              average kld loss:  46.8510\n",
      "              average accuracy:     0.6624\n",
      "              average auroc:  0.6619\n",
      "\n",
      "[epoch 024]  average training loss: 1065.4090\n",
      "\n",
      "Evaluation: \n",
      "[epoch 024]  average elbo loss: 1045.7238\n",
      "              average ce loss:   17.6371\n",
      "              average kld loss:  46.6422\n",
      "              average accuracy:     0.6683\n",
      "              average auroc:  0.6720\n",
      "\n",
      "[epoch 025]  average training loss: 1065.3181\n",
      "\n",
      "Evaluation: \n",
      "[epoch 025]  average elbo loss: 1062.0362\n",
      "              average ce loss:   17.8204\n",
      "              average kld loss:  51.1700\n",
      "              average accuracy:     0.6706\n",
      "              average auroc:  0.6748\n",
      "\n",
      "[epoch 026]  average training loss: 1065.2968\n",
      "\n",
      "Evaluation: \n",
      "[epoch 026]  average elbo loss: 1058.2088\n",
      "              average ce loss:   17.8330\n",
      "              average kld loss:  49.7532\n",
      "              average accuracy:     0.6688\n",
      "              average auroc:  0.6725\n",
      "\n",
      "[epoch 027]  average training loss: 1065.4226\n",
      "\n",
      "Evaluation: \n",
      "[epoch 027]  average elbo loss: 1055.8200\n",
      "              average ce loss:   17.7742\n",
      "              average kld loss:  49.5821\n",
      "              average accuracy:     0.6714\n",
      "              average auroc:  0.6759\n",
      "\n",
      "[epoch 028]  average training loss: 1065.3339\n",
      "\n",
      "Evaluation: \n",
      "[epoch 028]  average elbo loss: 1050.0231\n",
      "              average ce loss:   17.6618\n",
      "              average kld loss:  51.2686\n",
      "              average accuracy:     0.6710\n",
      "              average auroc:  0.6762\n",
      "\n",
      "[epoch 029]  average training loss: 1065.3026\n",
      "\n",
      "Evaluation: \n",
      "[epoch 029]  average elbo loss: 1047.6338\n",
      "              average ce loss:   17.6891\n",
      "              average kld loss:  50.8393\n",
      "              average accuracy:     0.6715\n",
      "              average auroc:  0.6783\n",
      "\n",
      "[epoch 030]  average training loss: 1065.2809\n",
      "\n",
      "Evaluation: \n",
      "[epoch 030]  average elbo loss: 1058.1967\n",
      "              average ce loss:   17.7897\n",
      "              average kld loss:  45.9255\n",
      "              average accuracy:     0.6714\n",
      "              average auroc:  0.6789\n",
      "\n",
      "[epoch 031]  average training loss: 1065.3013\n",
      "\n",
      "Evaluation: \n",
      "[epoch 031]  average elbo loss: 1054.7444\n",
      "              average ce loss:   17.7323\n",
      "              average kld loss:  52.1803\n",
      "              average accuracy:     0.6745\n",
      "              average auroc:  0.6874\n",
      "\n",
      "[epoch 032]  average training loss: 1065.1453\n",
      "\n",
      "Evaluation: \n",
      "[epoch 032]  average elbo loss: 1053.5785\n",
      "              average ce loss:   17.7546\n",
      "              average kld loss:  46.8790\n",
      "              average accuracy:     0.6757\n",
      "              average auroc:  0.6870\n",
      "\n",
      "[epoch 033]  average training loss: 1065.1960\n",
      "\n",
      "Evaluation: \n",
      "[epoch 033]  average elbo loss: 1055.3351\n",
      "              average ce loss:   17.7483\n",
      "              average kld loss:  51.3391\n",
      "              average accuracy:     0.6783\n",
      "              average auroc:  0.6898\n",
      "\n",
      "[epoch 034]  average training loss: 1065.1543\n",
      "\n",
      "Evaluation: \n",
      "[epoch 034]  average elbo loss: 1055.1695\n",
      "              average ce loss:   17.7271\n",
      "              average kld loss:  50.5290\n",
      "              average accuracy:     0.6726\n",
      "              average auroc:  0.6936\n",
      "\n",
      "[epoch 035]  average training loss: 1065.2365\n",
      "\n",
      "Evaluation: \n",
      "[epoch 035]  average elbo loss: 1060.9764\n",
      "              average ce loss:   17.8239\n",
      "              average kld loss:  55.3878\n",
      "              average accuracy:     0.6808\n",
      "              average auroc:  0.6970\n",
      "\n",
      "[epoch 036]  average training loss: 1065.0991\n",
      "\n",
      "Evaluation: \n",
      "[epoch 036]  average elbo loss: 1059.0857\n",
      "              average ce loss:   17.7767\n",
      "              average kld loss:  49.9170\n",
      "              average accuracy:     0.6760\n",
      "              average auroc:  0.6965\n",
      "\n",
      "[epoch 037]  average training loss: 1065.1790\n",
      "\n",
      "Evaluation: \n",
      "[epoch 037]  average elbo loss: 1053.0180\n",
      "              average ce loss:   17.7514\n",
      "              average kld loss:  47.4902\n",
      "              average accuracy:     0.6826\n",
      "              average auroc:  0.7019\n",
      "\n",
      "[epoch 038]  average training loss: 1065.2974\n",
      "\n",
      "Evaluation: \n",
      "[epoch 038]  average elbo loss: 1060.9800\n",
      "              average ce loss:   17.8028\n",
      "              average kld loss:  49.9126\n",
      "              average accuracy:     0.6818\n",
      "              average auroc:  0.6986\n",
      "\n",
      "[epoch 039]  average training loss: 1065.0481\n",
      "\n",
      "Evaluation: \n",
      "[epoch 039]  average elbo loss: 1050.4782\n",
      "              average ce loss:   17.7058\n",
      "              average kld loss:  48.6050\n",
      "              average accuracy:     0.6799\n",
      "              average auroc:  0.6996\n",
      "\n",
      "[epoch 040]  average training loss: 1065.3390\n",
      "\n",
      "Evaluation: \n",
      "[epoch 040]  average elbo loss: 1043.4528\n",
      "              average ce loss:   17.6242\n",
      "              average kld loss:  50.8603\n",
      "              average accuracy:     0.6850\n",
      "              average auroc:  0.7026\n",
      "\n",
      "[epoch 041]  average training loss: 1065.1878\n",
      "\n",
      "Evaluation: \n",
      "[epoch 041]  average elbo loss: 1055.8531\n",
      "              average ce loss:   17.7598\n",
      "              average kld loss:  47.6240\n",
      "              average accuracy:     0.6792\n",
      "              average auroc:  0.6976\n",
      "\n",
      "[epoch 042]  average training loss: 1065.0915\n",
      "\n",
      "Evaluation: \n",
      "[epoch 042]  average elbo loss: 1057.4323\n",
      "              average ce loss:   17.7660\n",
      "              average kld loss:  48.7106\n",
      "              average accuracy:     0.6846\n",
      "              average auroc:  0.7059\n",
      "\n",
      "[epoch 043]  average training loss: 1065.2429\n"
     ]
    }
   ],
   "source": [
    "pretrained = True\n",
    "frozen = False\n",
    "run_id = \"s23_noOF_logtheta10_simscale10_5prototypes_lr0003\"\n",
    "\n",
    "run_name_original = (\"pretrained_\" if pretrained else \"unpretrained_\") + (\"frozen_\" if frozen else \"unfrozen_\") + \"movies_\" + run_id\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "test_celoss = []\n",
    "test_klloss = []\n",
    "\n",
    "print(\"Beginning Training\")\n",
    "\n",
    "# training loop\n",
    "\n",
    "for p_percentage in [0.01]:\n",
    "    pyro.set_rng_seed(0)\n",
    "    run_name = run_name_original + f\"_lp{int(p_percentage * 100)}\"\n",
    "    print(f\"Training for percentage: {p_percentage}\")\n",
    "    best_accuracy = 0\n",
    "    run = wandb.init(project=\"prodLDA_pairs\", entity=\"witw\", )\n",
    "    wandb.run.name = run_name\n",
    "    pwd, dl_train, dl_val, dl_test = get_data(p_percentage)\n",
    "    \n",
    "    topic_model, svi = init_model(pretrained=pretrained, frozen=frozen, pwd=pwd)\n",
    "    wandb.watch(topic_model, log='all', log_freq=100)\n",
    "    trace_model(topic_model, dl_train=dl_train)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        trans = get_map(topic_model)\n",
    "        total_epoch_loss_train, _, _ = train(svi, topic_model, dl_train, device=DEVICE, print_debug=True, epoch=epoch, latest_plotted_p=latest_plotted_p, trans = trans)\n",
    "        train_elbo.append(-total_epoch_loss_train)\n",
    "\n",
    "        wandb.log({'epoch': epoch,\n",
    "                   'train_elbo': total_epoch_loss_train})\n",
    "\n",
    "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "        if epoch % TEST_FREQUENCY == 0:\n",
    "            # report test diagnostics\n",
    "            total_epoch_loss_test, total_epoch_celoss_test, total_epoch_klloss_test, test_acc, test_auroc = evaluate(svi, topic_model, dl_val, device=DEVICE)\n",
    "            test_elbo.append(-total_epoch_loss_test)\n",
    "            test_celoss.append(total_epoch_celoss_test)\n",
    "            test_klloss.append(total_epoch_klloss_test)\n",
    "    #         print(x)\n",
    "            wandb.log({'epoch': epoch,\n",
    "                       'test_acc': test_acc,\n",
    "                       'test_auroc': test_auroc,\n",
    "                       'test_elbo': total_epoch_loss_test,\n",
    "                       'test_entropy': total_epoch_celoss_test,\n",
    "                       'test_kl': total_epoch_klloss_test})\n",
    "\n",
    "            print(\"\\nEvaluation: \")\n",
    "            print(\"[epoch %03d]  average elbo loss: %.4f\" % (epoch, total_epoch_loss_test))\n",
    "            print(\"              average ce loss:   %.4f\" % (total_epoch_celoss_test))\n",
    "            print(\"              average kld loss:  %.4f\" % (total_epoch_klloss_test))\n",
    "            print(\"              average accuracy:     %.4f\" % (test_acc))\n",
    "            print(\"              average auroc:  %.4f\\n\" % (test_auroc))\n",
    "            if test_acc > best_accuracy:\n",
    "                best_accuracy = test_acc\n",
    "                torch.save({\"model\" : topic_model.state_dict()}, f\"../data/movie_checkpoints/labeled_percentage_sweep/bestmodel_\" + run_name + \".pt\")\n",
    "    torch.save({\"model\" : topic_model.state_dict()}, f\"../data/movie_checkpoints/labeled_percentage_sweep/lastmodel_\" + run_name + \".pt\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ProdLDA_semi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
