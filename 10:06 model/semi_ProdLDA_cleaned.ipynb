{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqhPcS1AyjuX"
   },
   "source": [
    "### Installs, Imports, and Pyro Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EkkTbMWyjuZ",
    "outputId": "e1cb369f-b60c-45a5-9a9f-c196a08adad5"
   },
   "outputs": [],
   "source": [
    "# 1# Run first time\n",
    "# ! module load cuda/9.2.88-gcc/7.1.0 cudnn/7.6.5.32-9.2-linux-x64-gcc/7.1.0-cuda9_2 anaconda3/2019.10-gcc/8.3.1\n",
    "! source activate pytorch_env\n",
    "# !pip install --up\n",
    "# !pip install pyro-ppl\n",
    "# !pip install torchvision\n",
    "# !pip install --upgrade git+https://github.com/dhudsmith/clean-the-text\n",
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W39rR1nNyjuZ"
   },
   "outputs": [],
   "source": [
    "from dataset import PairwiseData, DocumentPairData\n",
    "from model import Encoder, Decoder, ProdLDA, custom_elbo\n",
    "import os\n",
    "import pickle as pkl\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import graphviz\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import pyro\n",
    "from pyro import poutine\n",
    "import pyro.distributions as dist\n",
    "# import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "import torch.nn.functional as F\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import umap\n",
    "import umap.plot\n",
    "import sklearn.datasets\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LpuHjrWDyjua"
   },
   "outputs": [],
   "source": [
    "pyro.distributions.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1-TcWDrjyjua"
   },
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-Gm9gInyjua",
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fWiuASbwyjua"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "NUMBER_PAIRS = 100000\n",
    "PAIR_PERCENTAGE = 1.0\n",
    "\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Dropout rate\n",
    "NUM_TOPICS = 50\n",
    "NUM_PROTOTYPES = 7\n",
    "EMBED_DIM  = 64\n",
    "HIDDEN_DIM = 128\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# Training\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS = 25\n",
    "TEST_FREQUENCY = 1\n",
    "BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 2000\n",
    "OBS_SAMPLES = 10\n",
    "\n",
    "# gamma = 0.25\n",
    "# num_steps = (NUMBER_PAIRS // BATCH_SIZE) * NUM_EPOCHS\n",
    "# lrd = np.exp(np.log(gamma)/num_steps)\n",
    "adam_args = {\"lr\": 0.003, 'clip_norm':10.0, 'betas': (0.99, 0.999)}\n",
    "\n",
    "latest_plotted_p = torch.zeros(NUM_PROTOTYPES, NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhrFMmkEyjub",
    "tags": []
   },
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oHW-3swTyjue"
   },
   "outputs": [],
   "source": [
    "def get_data(pair_percentage=PAIR_PERCENTAGE):\n",
    "    # pairwise data\n",
    "    pwd = PairwiseData()\n",
    "    train_pairs, val_pairs, test_pairs = [pwd.get_pairs_table(d) for d in [pwd.train, pwd.val, pwd.test]]\n",
    "\n",
    "    # datasets\n",
    "    data_train, data_val, data_test = [\n",
    "        DocumentPairData(bows=pwd.bows, index_table=ix_table, prob=PAIR_PERCENTAGE)\n",
    "        for ix_table in [train_pairs, val_pairs, test_pairs]\n",
    "    ]\n",
    "\n",
    "    # dataloaders\n",
    "    dl_train = DataLoader(data_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "    dl_val = DataLoader(data_val, batch_size=VAL_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "    dl_test = DataLoader(data_test, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "    \n",
    "    return pwd, dl_train, dl_val, dl_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fk2anUsCyjuf"
   },
   "source": [
    "# Training and Evaluation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "q-wenFNRyjuf"
   },
   "outputs": [],
   "source": [
    "def train(svi, topic_model, train_loader, device, progress_interval = 100, print_debug = False, epoch = 0, latest_plotted_p = None, trans = None):\n",
    "    topic_model.train()\n",
    "    \n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    logits_epoch = []\n",
    "    labels_epoch = []\n",
    "    batches = 0\n",
    "    \n",
    "    \n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for i, x in enumerate(train_loader):\n",
    "        batches += 1\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        x_a = x['a'].to(device).squeeze()\n",
    "        x_b = x['b'].to(device).squeeze()\n",
    "        x_label = x['label'].to(device).type(torch.float32)\n",
    "        x_observed = x['observed'].to(device).type(torch.bool)\n",
    "        \n",
    "        if print_debug and i % progress_interval == 0:\n",
    "            print(f\"Step {i}; avg. loss {epoch_loss/(i+1)}\", end='\\r')\n",
    "            # p = pyro.param('p').detach().cpu()\n",
    "            # phi = torch.softmax(p, axis=-1)\n",
    "            # p_2d = trans.transform(p)\n",
    "            # latest_plotted_p[:] = p\n",
    "            # umap.plot.points(trans, labels=pwd.val.category,theme='fire')\n",
    "            # for ix, marker in enumerate(['$1$', '$2$', '$3$', '$4$', '$5$', '$6$', '$7$']):\n",
    "            #     plt.scatter(x=p_2d[ix,0], y=p_2d[ix,1], s=7**3, c='white', marker=marker)\n",
    "            # plt.title(f\"Frozen Enc/Dec : Epoch {epoch}, Step {i}\")\n",
    "            # plt.savefig(f'../data/figures_pretrained_unfrozen/epoch{epoch}_step{i}')\n",
    "            # plt.close()\n",
    "        \n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x_a, x_b, x_label, x_observed)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = svi.guide(x_a, x_b, x_label, x_observed)\n",
    "\n",
    "        x_label = x_label.cpu()\n",
    "        logits = x.cpu()\n",
    "        logits_epoch += logits\n",
    "        labels_epoch += x_label\n",
    "        \n",
    "    logits_binned = np.digitize(logits_epoch, [0.45], right=False)\n",
    "    train_acc = accuracy_score(labels_epoch, logits_binned)\n",
    "    train_auroc = roc_auc_score(labels_epoch, logits_epoch)\n",
    "    \n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train, train_acc, train_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "56ugiYHHyjuf"
   },
   "outputs": [],
   "source": [
    "def evaluate(svi, etm, test_loader, device): \n",
    "    etm.eval()\n",
    "    \n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.0\n",
    "    test_ce_loss = 0.0\n",
    "    test_kl_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_auroc = 0.0\n",
    "    batches = 0\n",
    "    logits_epoch = []\n",
    "    labels_epoch = []\n",
    "    \n",
    "    # compute the loss over the entire test set\n",
    "    for x in test_loader:\n",
    "        batches += 1\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        x_a = x['a'].to(device).squeeze()\n",
    "        x_b = x['b'].to(device).squeeze()\n",
    "        x_label = x['label'].to(device).type(torch.float32)\n",
    "        x_observed = x['observed'].to(device).type(torch.bool)\n",
    "            \n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x_a, x_b, x_label, x_observed)\n",
    "        \n",
    "        # generate reconstruction of batch documents and move to cuda if designated\n",
    "        with torch.no_grad():\n",
    "            recon_x_a = etm.reconstruct_doc(x_a)\n",
    "            recon_x_b = etm.reconstruct_doc(x_b)\n",
    "            logits = svi.guide(x_a, x_b, x_label, x_observed)\n",
    "\n",
    "        x_label = x_label.cpu()\n",
    "        logits = logits.cpu()\n",
    "        logits_epoch += logits\n",
    "        labels_epoch += x_label\n",
    "\n",
    "        # calculate and sum cross entropy loss and kl divergence\n",
    "        x_a=x_a.squeeze()\n",
    "        x_b=x_b.squeeze()\n",
    "        log_probs_a = torch.log(recon_x_a)\n",
    "        log_probs_b = torch.log(recon_x_b)\n",
    "        \n",
    "        if log_probs_a.isnan().any() | log_probs_b.isnan().any() | (log_probs_a.abs() > 30).any() | (log_probs_b.abs() > 30).any():\n",
    "            raise ValueError(\"nan or very large log probs\")\n",
    "            \n",
    "        targets_a = x_a/(x_a.sum(axis=-1)[:,None])\n",
    "        targets_b = x_b/(x_b.sum(axis=-1)[:,None])\n",
    "        ce_loss_a = F.cross_entropy(log_probs_a, targets_a, reduction='sum')\n",
    "        ce_loss_b = F.cross_entropy(log_probs_b, targets_b, reduction='sum')\n",
    "        test_ce_loss += ce_loss_a + ce_loss_b\n",
    "        test_kl_loss += etm.calc_kl_divergence(x_a.squeeze(1)) + etm.calc_kl_divergence(x_b.squeeze(1))\n",
    "\n",
    "        \n",
    "    logits_binned = np.digitize(logits_epoch, [0.45], right=False)\n",
    "    test_acc = accuracy_score(labels_epoch, logits_binned)\n",
    "    test_auroc = roc_auc_score(labels_epoch, logits_epoch)\n",
    "    \n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    epoch_elbo = test_loss / normalizer_test\n",
    "    epoch_ce_loss = test_ce_loss / normalizer_test\n",
    "    epoch_kl_loss = test_kl_loss / normalizer_test\n",
    "    \n",
    "    return epoch_elbo, epoch_ce_loss.item(), epoch_kl_loss.item(), test_acc, test_auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsaotgwIyjuf"
   },
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(pretrained, frozen, pwd):\n",
    "    try:\n",
    "        if frozen and not pretrained:\n",
    "            raise Exception\n",
    "    except:\n",
    "        print('ERROR: attempting to freeze an non-pretrained model!')\n",
    "        \n",
    "    # clear param store\n",
    "    pyro.clear_param_store()\n",
    "    # setup the VAE\n",
    "    topic_model = ProdLDA(pwd.vocab_size, NUM_TOPICS, NUM_PROTOTYPES, HIDDEN_DIM, DROPOUT_RATE, DEVICE, frozen=frozen).to(DEVICE)\n",
    "    \n",
    "    if pretrained:\n",
    "        saved_model_dict = torch.load(\"../data/pretrained_lda_5.pt\")\n",
    "        topic_model.load_state_dict(saved_model_dict['model'])\n",
    "        # svi_guide = saved_model_dict['guide']\n",
    "        pyro.get_param_store().load(\"../data/pretrained_params_5.pt\")\n",
    "    \n",
    "    if frozen:\n",
    "        for param in topic_model.encoder.parameters():\n",
    "            param.requires_grad=False\n",
    "        for param in topic_model.decoder.parameters():\n",
    "            param.requires_grad=False\n",
    "        topic_model.frozen = True\n",
    "        \n",
    "    # setup the optimizer\n",
    "    optimizer = ClippedAdam(adam_args)\n",
    "    # setup the inference algorithm\n",
    "    svi = SVI(topic_model.model, topic_model.guide, optimizer, loss=custom_elbo)\n",
    "    \n",
    "\n",
    "    \n",
    "    return topic_model, svi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_model(topic_model, dl_train):\n",
    "    batch_1 = next(iter(dl_train))\n",
    "    dat_a = batch_1['a'].to(DEVICE).squeeze()\n",
    "    dat_b = batch_1['b'].to(DEVICE).squeeze()\n",
    "    dat_label = batch_1['label'].to(DEVICE).squeeze().type(torch.float32)\n",
    "    dat_observed = batch_1['observed'].to(DEVICE).squeeze().type(torch.bool)\n",
    "\n",
    "    print(pyro.poutine.trace(topic_model.model).get_trace(dat_a, dat_b, dat_label, dat_observed).format_shapes())\n",
    "    print(pyro.poutine.trace(topic_model.guide).get_trace(dat_a, dat_b, dat_label, dat_observed).format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map(topic_model):\n",
    "    x = torch.tensor(pwd.bows_val.toarray()).to(DEVICE).type(torch.float)\n",
    "    with torch.no_grad():\n",
    "        z_loc, z_scale = topic_model.encoder(x)\n",
    "\n",
    "    features = z_loc.cpu()\n",
    "    features = torch.softmax(features, axis=-1)\n",
    "    features = features.numpy()\n",
    "\n",
    "    trans = umap.UMAP(n_neighbors=15, random_state=42, min_dist=0.1).fit(features)\n",
    "    return trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vHKVjjOCyjug",
    "outputId": "ea1176e6-2e15-4934-f8ff-d1bfb18a0b31",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training\n",
      "Training for percentage: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwitw\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/witw/prodLDA_pairs/runs/q2sn7dmh\" target=\"_blank\">dry-sky-169</a></strong> to <a href=\"https://wandb.ai/witw/prodLDA_pairs\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Trace Shapes:             \n",
      "         Param Sites:             \n",
      "decoder$$$beta.weight 2000 50     \n",
      "        Sample Sites:             \n",
      "     documents_a dist       |     \n",
      "                value  128  |     \n",
      "      logtheta_a dist  128  |   50\n",
      "                value  128  |   50\n",
      "           obs_a dist  128  | 2000\n",
      "                value  128  | 2000\n",
      "     documents_b dist       |     \n",
      "                value  128  |     \n",
      "      logtheta_b dist  128  |   50\n",
      "                value  128  |   50\n",
      "           obs_b dist  128  | 2000\n",
      "                value  128  | 2000\n",
      "      prototypes dist    7  |   50\n",
      "                value    7  |   50\n",
      "        Trace Shapes:             \n",
      "         Param Sites:             \n",
      " encoder$$$fc1.weight 128 2000    \n",
      "   encoder$$$fc1.bias      128    \n",
      " encoder$$$fc2.weight 128  128    \n",
      "   encoder$$$fc2.bias      128    \n",
      "encoder$$$fcmu.weight  50  128    \n",
      "  encoder$$$fcmu.bias       50    \n",
      "encoder$$$fclv.weight  50  128    \n",
      "  encoder$$$fclv.bias       50    \n",
      "                    p   7   50    \n",
      "                    a             \n",
      "                    b             \n",
      "        Sample Sites:             \n",
      "     documents_a dist        |    \n",
      "                value 128    |    \n",
      "      logtheta_a dist 128    |  50\n",
      "                value 128    |  50\n",
      "     documents_b dist        |    \n",
      "                value 128    |    \n",
      "      logtheta_b dist 128    |  50\n",
      "                value 128    |  50\n",
      "      prototypes dist   7    |  50\n",
      "                value   7    |  50\n",
      "               x dist        | 128\n",
      "                value        | 128\n",
      "[epoch 000]  average training loss: 449.2190\n",
      "\n",
      "Evaluation: \n",
      "[epoch 000]  average elbo loss: 408.7575\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  50.0146\n",
      "              average accuracy:     0.8065\n",
      "              average auroc:  0.5418\n",
      "\n",
      "[epoch 001]  average training loss: 444.4556\n",
      "\n",
      "Evaluation: \n",
      "[epoch 001]  average elbo loss: 412.8943\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  53.7686\n",
      "              average accuracy:     0.8186\n",
      "              average auroc:  0.7593\n",
      "\n",
      "[epoch 002]  average training loss: 444.0235\n",
      "\n",
      "Evaluation: \n",
      "[epoch 002]  average elbo loss: 415.6894\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  59.1691\n",
      "              average accuracy:     0.8314\n",
      "              average auroc:  0.8099\n",
      "\n",
      "[epoch 003]  average training loss: 444.3465\n",
      "\n",
      "Evaluation: \n",
      "[epoch 003]  average elbo loss: 410.7692\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  55.7111\n",
      "              average accuracy:     0.8318\n",
      "              average auroc:  0.7995\n",
      "\n",
      "[epoch 004]  average training loss: 443.7729\n",
      "\n",
      "Evaluation: \n",
      "[epoch 004]  average elbo loss: 410.7734\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  54.0175\n",
      "              average accuracy:     0.8296\n",
      "              average auroc:  0.8004\n",
      "\n",
      "[epoch 005]  average training loss: 444.7406\n",
      "\n",
      "Evaluation: \n",
      "[epoch 005]  average elbo loss: 409.4409\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  50.1327\n",
      "              average accuracy:     0.8275\n",
      "              average auroc:  0.8163\n",
      "\n",
      "[epoch 006]  average training loss: 444.0474\n",
      "\n",
      "Evaluation: \n",
      "[epoch 006]  average elbo loss: 411.4207\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  56.8085\n",
      "              average accuracy:     0.8325\n",
      "              average auroc:  0.7976\n",
      "\n",
      "[epoch 007]  average training loss: 443.7352\n",
      "\n",
      "Evaluation: \n",
      "[epoch 007]  average elbo loss: 406.2604\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  44.4064\n",
      "              average accuracy:     0.8289\n",
      "              average auroc:  0.8109\n",
      "\n",
      "[epoch 008]  average training loss: 444.3748\n",
      "\n",
      "Evaluation: \n",
      "[epoch 008]  average elbo loss: 411.0807\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  55.4807\n",
      "              average accuracy:     0.8291\n",
      "              average auroc:  0.8019\n",
      "\n",
      "[epoch 009]  average training loss: 443.8253\n",
      "\n",
      "Evaluation: \n",
      "[epoch 009]  average elbo loss: 411.9704\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  54.6740\n",
      "              average accuracy:     0.8286\n",
      "              average auroc:  0.8079\n",
      "\n",
      "[epoch 010]  average training loss: 444.5891\n",
      "\n",
      "Evaluation: \n",
      "[epoch 010]  average elbo loss: 409.2531\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  50.6135\n",
      "              average accuracy:     0.8306\n",
      "              average auroc:  0.8061\n",
      "\n",
      "[epoch 011]  average training loss: 443.9981\n",
      "\n",
      "Evaluation: \n",
      "[epoch 011]  average elbo loss: 411.1651\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  52.5886\n",
      "              average accuracy:     0.8313\n",
      "              average auroc:  0.8103\n",
      "\n",
      "[epoch 012]  average training loss: 443.6625\n",
      "\n",
      "Evaluation: \n",
      "[epoch 012]  average elbo loss: 408.0491\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  47.8535\n",
      "              average accuracy:     0.8294\n",
      "              average auroc:  0.8132\n",
      "\n",
      "[epoch 013]  average training loss: 443.7680\n",
      "\n",
      "Evaluation: \n",
      "[epoch 013]  average elbo loss: 409.8717\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  52.8207\n",
      "              average accuracy:     0.8302\n",
      "              average auroc:  0.7966\n",
      "\n",
      "[epoch 014]  average training loss: 444.6651\n",
      "\n",
      "Evaluation: \n",
      "[epoch 014]  average elbo loss: 408.0851\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  45.3796\n",
      "              average accuracy:     0.8263\n",
      "              average auroc:  0.8093\n",
      "\n",
      "[epoch 015]  average training loss: 443.9551\n",
      "\n",
      "Evaluation: \n",
      "[epoch 015]  average elbo loss: 411.8991\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  55.4193\n",
      "              average accuracy:     0.8301\n",
      "              average auroc:  0.7925\n",
      "\n",
      "[epoch 016]  average training loss: 444.2009\n",
      "\n",
      "Evaluation: \n",
      "[epoch 016]  average elbo loss: 411.3094\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  55.1342\n",
      "              average accuracy:     0.8292\n",
      "              average auroc:  0.8004\n",
      "\n",
      "[epoch 017]  average training loss: 444.3775\n",
      "\n",
      "Evaluation: \n",
      "[epoch 017]  average elbo loss: 410.0200\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  49.5693\n",
      "              average accuracy:     0.8289\n",
      "              average auroc:  0.8084\n",
      "\n",
      "[epoch 018]  average training loss: 444.3535\n",
      "\n",
      "Evaluation: \n",
      "[epoch 018]  average elbo loss: 410.6058\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  53.5667\n",
      "              average accuracy:     0.8315\n",
      "              average auroc:  0.8048\n",
      "\n",
      "[epoch 019]  average training loss: 443.9041\n",
      "\n",
      "Evaluation: \n",
      "[epoch 019]  average elbo loss: 409.1793\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  52.2005\n",
      "              average accuracy:     0.8297\n",
      "              average auroc:  0.8001\n",
      "\n",
      "[epoch 020]  average training loss: 445.0169\n",
      "\n",
      "Evaluation: \n",
      "[epoch 020]  average elbo loss: 413.1139\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  57.2994\n",
      "              average accuracy:     0.8309\n",
      "              average auroc:  0.8009\n",
      "\n",
      "[epoch 021]  average training loss: 444.5230\n",
      "\n",
      "Evaluation: \n",
      "[epoch 021]  average elbo loss: 413.5631\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  53.1250\n",
      "              average accuracy:     0.8278\n",
      "              average auroc:  0.8075\n",
      "\n",
      "[epoch 022]  average training loss: 443.7430\n",
      "\n",
      "Evaluation: \n",
      "[epoch 022]  average elbo loss: 409.0260\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  51.0231\n",
      "              average accuracy:     0.8277\n",
      "              average auroc:  0.8090\n",
      "\n",
      "[epoch 023]  average training loss: 444.0880\n",
      "\n",
      "Evaluation: \n",
      "[epoch 023]  average elbo loss: 414.6442\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  54.6415\n",
      "              average accuracy:     0.8294\n",
      "              average auroc:  0.8067\n",
      "\n",
      "[epoch 024]  average training loss: 443.7804\n",
      "\n",
      "Evaluation: \n",
      "[epoch 024]  average elbo loss: 412.2002\n",
      "              average ce loss:   nan\n",
      "              average kld loss:  59.0499\n",
      "              average accuracy:     0.8350\n",
      "              average auroc:  0.8061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_elbo = []\n",
    "test_elbo = []\n",
    "test_celoss = []\n",
    "test_klloss = []\n",
    "\n",
    "print(\"Beginning Training\")\n",
    "\n",
    "# training loop\n",
    "\n",
    "for p_percentage in [1.0]:\n",
    "    print(f\"Training for percentage: {p_percentage}\")\n",
    "    best_elbo = 10**10\n",
    "    run = wandb.init(project=\"prodLDA_pairs\", entity=\"witw\", )\n",
    "    wandb.run.name = f\"pretrained_unfrozen_2000vocab_percentage{p_percentage}_2\" \n",
    "    pwd, dl_train, dl_val, dl_test = get_data(p_percentage)\n",
    "    \n",
    "    topic_model, svi = init_model(pretrained=True, frozen=False, pwd=pwd)\n",
    "    wandb.watch(topic_model, log=['gradients', 'parameters'], log_freq=100)\n",
    "    trace_model(topic_model, dl_train=dl_train)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        trans = get_map(topic_model)\n",
    "        total_epoch_loss_train, _, _ = train(svi, topic_model, dl_train, device=DEVICE, print_debug=True, epoch=epoch, latest_plotted_p=latest_plotted_p, trans = trans)\n",
    "        train_elbo.append(-total_epoch_loss_train)\n",
    "\n",
    "        wandb.log({'epoch': epoch,\n",
    "                   'train_elbo': total_epoch_loss_train})\n",
    "\n",
    "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "        if epoch % TEST_FREQUENCY == 0:\n",
    "            # report test diagnostics\n",
    "            total_epoch_loss_test, total_epoch_celoss_test, total_epoch_klloss_test, test_acc, test_auroc = evaluate(svi, topic_model, dl_val, device=DEVICE)\n",
    "            test_elbo.append(-total_epoch_loss_test)\n",
    "            test_celoss.append(total_epoch_celoss_test)\n",
    "            test_klloss.append(total_epoch_klloss_test)\n",
    "    #         print(x)\n",
    "            wandb.log({'epoch': epoch,\n",
    "                       'test_acc': test_acc,\n",
    "                       'test_auroc': test_auroc,\n",
    "                       'test_elbo': total_epoch_loss_test,\n",
    "                       'test_entropy': total_epoch_celoss_test,\n",
    "                       'test_kl': total_epoch_klloss_test})\n",
    "\n",
    "            print(\"\\nEvaluation: \")\n",
    "            print(\"[epoch %03d]  average elbo loss: %.4f\" % (epoch, total_epoch_loss_test))\n",
    "            print(\"              average ce loss:   %.4f\" % (total_epoch_celoss_test))\n",
    "            print(\"              average kld loss:  %.4f\" % (total_epoch_klloss_test))\n",
    "            print(\"              average accuracy:     %.4f\" % (test_acc))\n",
    "            print(\"              average auroc:  %.4f\\n\" % (test_auroc))\n",
    "            if total_epoch_loss_test < best_elbo:\n",
    "                best_elbo = total_epoch_loss_test\n",
    "                torch.save({\"model\" : topic_model.state_dict()}, f\"../data/bestmodel2000P{p_percentage}_unfrozen_2.pt\")\n",
    "                pyro.get_param_store().save(f\"../data/bestmodelparams2000P{p_percentage}_unfrozen_2.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model\" : topic_model.state_dict()}, f\"../data/bestmodel2000P{p_percentage}_unfrozen_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ProdLDA.guide of ProdLDA(\n",
       "  (encoder): Encoder(\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (fc1): Linear(in_features=2000, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fcmu): Linear(in_features=128, out_features=50, bias=True)\n",
       "    (fclv): Linear(in_features=128, out_features=50, bias=True)\n",
       "    (bnmu): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (bnlv): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (beta): Linear(in_features=50, out_features=2000, bias=False)\n",
       "    (bn): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svi.guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ProdLDA_semi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
